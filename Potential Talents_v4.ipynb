{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.load import load_data\n",
    "from utils.split import split_data\n",
    "from utils.process_text import process_text, convert_terms, convert_words_to_vectors, get_word_vectors\n",
    "from utils.predict import get_rank_predictions\n",
    "from utils.evaluate import cosine_similarity\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models.fasttext import FastText\n",
    "from xgboost import XGBRanker\n",
    "from lightgbm import LGBMRanker\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 104 entries, 0 to 103\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   id          104 non-null    int64  \n",
      " 1   job_title   104 non-null    object \n",
      " 2   location    104 non-null    object \n",
      " 3   connection  104 non-null    object \n",
      " 4   fit         0 non-null      float64\n",
      "dtypes: float64(1), int64(1), object(3)\n",
      "memory usage: 4.2+ KB\n",
      "None \n",
      "\n",
      "               id  fit\n",
      "count  104.000000  0.0\n",
      "mean    52.500000  NaN\n",
      "std     30.166206  NaN\n",
      "min      1.000000  NaN\n",
      "25%     26.750000  NaN\n",
      "50%     52.500000  NaN\n",
      "75%     78.250000  NaN\n",
      "max    104.000000  NaN \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>connection</th>\n",
       "      <th>fit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2019 C.T. Bauer College of Business Graduate (...</td>\n",
       "      <td>Houston, Texas</td>\n",
       "      <td>85</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Native English Teacher at EPIK (English Progra...</td>\n",
       "      <td>Kanada</td>\n",
       "      <td>500+</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Aspiring Human Resources Professional</td>\n",
       "      <td>Raleigh-Durham, North Carolina Area</td>\n",
       "      <td>44</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>People Development Coordinator at Ryan</td>\n",
       "      <td>Denton, Texas</td>\n",
       "      <td>500+</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Advisory Board Member at Celal Bayar University</td>\n",
       "      <td>İzmir, Türkiye</td>\n",
       "      <td>500+</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                          job_title  \\\n",
       "0   1  2019 C.T. Bauer College of Business Graduate (...   \n",
       "1   2  Native English Teacher at EPIK (English Progra...   \n",
       "2   3              Aspiring Human Resources Professional   \n",
       "3   4             People Development Coordinator at Ryan   \n",
       "4   5    Advisory Board Member at Celal Bayar University   \n",
       "\n",
       "                              location connection  fit  \n",
       "0                       Houston, Texas         85  NaN  \n",
       "1                               Kanada      500+   NaN  \n",
       "2  Raleigh-Durham, North Carolina Area         44  NaN  \n",
       "3                        Denton, Texas      500+   NaN  \n",
       "4                       İzmir, Türkiye      500+   NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_data(file_name=\"potential-talents.xlsx\", folder_name=\"data\")\n",
    "print(data.info(), \"\\n\")\n",
    "print(data.describe(), \"\\n\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ProfileReport(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The id column is just an index column that would not be relevant to the fitness of any roles.\n",
    "Although the job_title and location columns are highly correlated, the job_title column seems to be the only relevant column in determining the fitness of a particular role based on the column values and information we have about the requirements.\n",
    "\n",
    "Therefore, only the job_title column will be used in the ranking procedures. Having said that, the other columns will still be returned in the result so that the user (i.e. the client) can have the full information about each of the relevant candidates.\n",
    "The fit column will be filled with a fitness score for each row/candidate later."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pre-process job titles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert human resources-related terms in a way that job titles containing those terms will have better fitness scores. That is, those job titles might end up having a fitness score of 0 without conversion because for instance \"HR\" and \"Human Resources\" would be considered to have nothing in common by most algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HR', 'SPHR', 'HRIS', 'CHRO,', 'GPHR']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_title_words = list(set(\" \".join(data['job_title']).split()))\n",
    "hr_words = [word for word in job_title_words if \"HR\" in word]\n",
    "hr_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_terms_dict = {'CHRO,': 'Chief Human Resources Officer,',\n",
    "                'GPHR': 'Global Professional in Human Resources',\n",
    "                'SPHR': 'Senior Professional in Human Resources',\n",
    "                'HR': 'Human Resources',\n",
    "                'HRIS': 'Human Resources Information System',\n",
    "                'People': 'Human'} # this is for titles like 'People Development Coordinator at Ryan'.\n",
    "\n",
    "for i, job_title in enumerate(data['job_title']):\n",
    "    converted = []\n",
    "    for word in job_title.split():\n",
    "        converted.append(convert_terms(word, hr_terms_dict))\n",
    "    data.loc[i, 'job_title'] = \" \".join(converted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar conversions can be done for terms like \"staff*\", \"employ*\", but we will leave the decision to domain experts and only convert terms that specifically include \"HR\" as above."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get fitness scores\n",
    "based on cosine similary between job titles and keywords using different algorithms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some descriptions about tfidf to be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_args = {'strip_accents':'unicode',\n",
    "              'lowercase':True,\n",
    "              'stop_words':'english',\n",
    "              'ngram_range':(1,3)}\n",
    "tfidf_vectorizer = TfidfVectorizer(**tfidf_args)\n",
    "\n",
    "job_title_processed_tfidf = data['job_title'].apply(\n",
    "    process_text,\n",
    "    remove_stopwords=True,\n",
    "    lemmatize=True,\n",
    "    stem=True\n",
    ")\n",
    "\n",
    "keywords = [\"Aspiring human resources\", \"seeking human resources\"]\n",
    "keywords_processed_tfidf = [process_text(keyword) for keyword in keywords]\n",
    "\n",
    "data['fit_tfidf'] = cosine_similarity(tfidf_vectorizer.fit_transform(job_title_processed_tfidf),\n",
    "                                      tfidf_vectorizer.transform(keywords_processed_tfidf)).sum(axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For other vectorizers, only remove stopwords without lemmatization or stemming since stopwords do not add any values/meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title_processed = data['job_title'].apply(\n",
    "    process_text,\n",
    "    remove_stopwords=True,\n",
    "    lemmatize=False,\n",
    "    stem=False\n",
    ")\n",
    "keywords_processed = [process_text(\n",
    "    keyword,\n",
    "    remove_stopwords=True,\n",
    "    lemmatize=False,\n",
    "    stem=False\n",
    "    ) for keyword in keywords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some descriptions about tensorflow.keras Tokenizer to be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(job_title_processed) # fit_on_texts updates internal vocabulary based on a list of texts; similar to tf-idf.\n",
    "data['fit_keras_tokenizer'] = cosine_similarity(tokenizer.texts_to_matrix(job_title_processed),\n",
    "                                                tokenizer.texts_to_matrix(keywords_processed)).sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some descriptions about gensim, glove, and word2vec to be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove file source: https://nlp.stanford.edu/projects/glove/\n",
    "word2vec_file = get_tmpfile('word2vec.6B.50d.txt') # Create a temp file\n",
    "glove2word2vec('data/glove/glove.6B.50d.txt', word2vec_file) # Save glove2word2vec into the temp file\n",
    "glove_vectors = KeyedVectors.load_word2vec_format(word2vec_file) # Load the glove2word2vec from the teamp file\n",
    "glove_dimension = 50\n",
    "\n",
    "# Transform job titles and keywords into glove vectors\n",
    "glove_vectors_job_title = convert_words_to_vectors(job_title_processed, glove_vectors, glove_dimension)\n",
    "glove_vectors_keywords = convert_words_to_vectors(keywords_processed, glove_vectors, glove_dimension)\n",
    "data['fit_glove'] = cosine_similarity(glove_vectors_job_title, glove_vectors_keywords).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(sentences=job_title_processed.apply(lambda x: [word.lower() for word in x.split()]))\n",
    "word2vec_dimension = word2vec.vector_size\n",
    "\n",
    "# Transform job titles and keywords into word2vec vectors\n",
    "word2vec_job_title = convert_words_to_vectors(job_title_processed, word2vec, word2vec_dimension)\n",
    "word2vec_keywords = convert_words_to_vectors(keywords_processed, word2vec, word2vec_dimension)\n",
    "data['fit_word2vec'] = cosine_similarity(word2vec_job_title, word2vec_keywords).sum(axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the disadvantage of GloVe embedding?\n",
    "One of the main disadvantages of Word2Vec and GloVe embedding is that they are unable to encode unknown or out-of-vocabulary words. So, to deal with this problem Facebook proposed a model FastText. It is an extension to Word2Vec and follows the same Skip-gram and CBOW model.\n",
    "\n",
    "some descriptions about fasttext to be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext = FastText(sentences=job_title_processed.apply(lambda x: [word.lower() for word in x.split()]))\n",
    "fasttext_dimension = fasttext.vector_size\n",
    "\n",
    "# Transform job titles and keywords into fasttext vectors\n",
    "fasttext_job_title = convert_words_to_vectors(job_title_processed, fasttext, fasttext_dimension)\n",
    "fasttext_keywords = convert_words_to_vectors(keywords_processed, fasttext, fasttext_dimension)\n",
    "data['fit_fasttext'] = cosine_similarity(fasttext_job_title, fasttext_keywords).sum(axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform fit scores so that different fit scores will have the same range between 0 and 1.<br>\n",
    "This is for easier comparisons among different fit scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>fit</th>\n",
       "      <th>fit_tfidf</th>\n",
       "      <th>fit_keras_tokenizer</th>\n",
       "      <th>fit_glove</th>\n",
       "      <th>fit_word2vec</th>\n",
       "      <th>fit_fasttext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>104.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>104.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>52.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.328938</td>\n",
       "      <td>0.541808</td>\n",
       "      <td>0.604935</td>\n",
       "      <td>0.594117</td>\n",
       "      <td>0.586781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>30.166206</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.315271</td>\n",
       "      <td>0.359278</td>\n",
       "      <td>0.313345</td>\n",
       "      <td>0.322589</td>\n",
       "      <td>0.290476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>26.750000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.057644</td>\n",
       "      <td>0.106066</td>\n",
       "      <td>0.286359</td>\n",
       "      <td>0.282529</td>\n",
       "      <td>0.422400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>52.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.253802</td>\n",
       "      <td>0.642826</td>\n",
       "      <td>0.664575</td>\n",
       "      <td>0.720907</td>\n",
       "      <td>0.684321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>78.250000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.497634</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.864163</td>\n",
       "      <td>0.845231</td>\n",
       "      <td>0.776623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>104.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id  fit   fit_tfidf  fit_keras_tokenizer   fit_glove  \\\n",
       "count  104.000000  0.0  104.000000           104.000000  104.000000   \n",
       "mean    52.500000  NaN    0.328938             0.541808    0.604935   \n",
       "std     30.166206  NaN    0.315271             0.359278    0.313345   \n",
       "min      1.000000  NaN    0.000000             0.000000    0.000000   \n",
       "25%     26.750000  NaN    0.057644             0.106066    0.286359   \n",
       "50%     52.500000  NaN    0.253802             0.642826    0.664575   \n",
       "75%     78.250000  NaN    0.497634             0.800000    0.864163   \n",
       "max    104.000000  NaN    1.000000             1.000000    1.000000   \n",
       "\n",
       "       fit_word2vec  fit_fasttext  \n",
       "count    104.000000    104.000000  \n",
       "mean       0.594117      0.586781  \n",
       "std        0.322589      0.290476  \n",
       "min        0.000000      0.000000  \n",
       "25%        0.282529      0.422400  \n",
       "50%        0.720907      0.684321  \n",
       "75%        0.845231      0.776623  \n",
       "max        1.000000      1.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minmax_scaler = MinMaxScaler()\n",
    "fit_columns = [col for col in data.columns if \"fit_\" in col]\n",
    "data[fit_columns] = minmax_scaler.fit_transform(data[fit_columns])\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['fit'] = data[fit_columns].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: job_title, dtype: int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['fit']==0].job_title.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>connection</th>\n",
       "      <th>fit</th>\n",
       "      <th>fit_tfidf</th>\n",
       "      <th>fit_keras_tokenizer</th>\n",
       "      <th>fit_glove</th>\n",
       "      <th>fit_word2vec</th>\n",
       "      <th>fit_fasttext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Seeking Human Resources Opportunities</td>\n",
       "      <td>Chicago, Illinois</td>\n",
       "      <td>390</td>\n",
       "      <td>4.821504</td>\n",
       "      <td>0.921024</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966611</td>\n",
       "      <td>0.933869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>Seeking Human Resources Opportunities</td>\n",
       "      <td>Chicago, Illinois</td>\n",
       "      <td>390</td>\n",
       "      <td>4.821504</td>\n",
       "      <td>0.921024</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966611</td>\n",
       "      <td>0.933869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>Seeking Human Resources Position</td>\n",
       "      <td>Las Vegas, Nevada Area</td>\n",
       "      <td>48</td>\n",
       "      <td>4.755860</td>\n",
       "      <td>0.913385</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.953443</td>\n",
       "      <td>0.966611</td>\n",
       "      <td>0.922421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>Aspiring Human Resources Professional</td>\n",
       "      <td>Raleigh-Durham, North Carolina Area</td>\n",
       "      <td>44</td>\n",
       "      <td>4.752130</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.920193</td>\n",
       "      <td>0.874349</td>\n",
       "      <td>0.957589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Aspiring Human Resources Professional</td>\n",
       "      <td>Raleigh-Durham, North Carolina Area</td>\n",
       "      <td>44</td>\n",
       "      <td>4.752130</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.920193</td>\n",
       "      <td>0.874349</td>\n",
       "      <td>0.957589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                              job_title  \\\n",
       "27  28  Seeking Human Resources Opportunities   \n",
       "29  30  Seeking Human Resources Opportunities   \n",
       "98  99       Seeking Human Resources Position   \n",
       "32  33  Aspiring Human Resources Professional   \n",
       "2    3  Aspiring Human Resources Professional   \n",
       "\n",
       "                               location connection       fit  fit_tfidf  \\\n",
       "27                    Chicago, Illinois        390  4.821504   0.921024   \n",
       "29                    Chicago, Illinois        390  4.821504   0.921024   \n",
       "98               Las Vegas, Nevada Area         48  4.755860   0.913385   \n",
       "32  Raleigh-Durham, North Carolina Area         44  4.752130   1.000000   \n",
       "2   Raleigh-Durham, North Carolina Area         44  4.752130   1.000000   \n",
       "\n",
       "    fit_keras_tokenizer  fit_glove  fit_word2vec  fit_fasttext  \n",
       "27                  1.0   1.000000      0.966611      0.933869  \n",
       "29                  1.0   1.000000      0.966611      0.933869  \n",
       "98                  1.0   0.953443      0.966611      0.922421  \n",
       "32                  1.0   0.920193      0.874349      0.957589  \n",
       "2                   1.0   0.920193      0.874349      0.957589  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sort_values('fit', ascending=False).head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the job titles for the candidates who got the highest fitness score, they indeed look very relevant - in fact the job titles include one of the exact keywords \"Aspiring Human Resources\" in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>connection</th>\n",
       "      <th>fit</th>\n",
       "      <th>fit_tfidf</th>\n",
       "      <th>fit_keras_tokenizer</th>\n",
       "      <th>fit_glove</th>\n",
       "      <th>fit_word2vec</th>\n",
       "      <th>fit_fasttext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>95</td>\n",
       "      <td>Student at Westfield State University</td>\n",
       "      <td>Bridgewater, Massachusetts</td>\n",
       "      <td>57</td>\n",
       "      <td>0.243059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.117990</td>\n",
       "      <td>0.125069</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>54</td>\n",
       "      <td>Student at Chapman University</td>\n",
       "      <td>Lake Forest, California</td>\n",
       "      <td>2</td>\n",
       "      <td>0.288568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032275</td>\n",
       "      <td>0.125069</td>\n",
       "      <td>0.131224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>63</td>\n",
       "      <td>Student at Chapman University</td>\n",
       "      <td>Lake Forest, California</td>\n",
       "      <td>2</td>\n",
       "      <td>0.288568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032275</td>\n",
       "      <td>0.125069</td>\n",
       "      <td>0.131224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41</td>\n",
       "      <td>Student at Chapman University</td>\n",
       "      <td>Lake Forest, California</td>\n",
       "      <td>2</td>\n",
       "      <td>0.288568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032275</td>\n",
       "      <td>0.125069</td>\n",
       "      <td>0.131224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Student at Chapman University</td>\n",
       "      <td>Lake Forest, California</td>\n",
       "      <td>2</td>\n",
       "      <td>0.288568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032275</td>\n",
       "      <td>0.125069</td>\n",
       "      <td>0.131224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                              job_title                    location  \\\n",
       "94  95  Student at Westfield State University  Bridgewater, Massachusetts   \n",
       "53  54          Student at Chapman University     Lake Forest, California   \n",
       "62  63          Student at Chapman University     Lake Forest, California   \n",
       "40  41          Student at Chapman University     Lake Forest, California   \n",
       "10  11          Student at Chapman University     Lake Forest, California   \n",
       "\n",
       "   connection       fit  fit_tfidf  fit_keras_tokenizer  fit_glove  \\\n",
       "94         57  0.243059        0.0                  0.0   0.117990   \n",
       "53          2  0.288568        0.0                  0.0   0.032275   \n",
       "62          2  0.288568        0.0                  0.0   0.032275   \n",
       "40          2  0.288568        0.0                  0.0   0.032275   \n",
       "10          2  0.288568        0.0                  0.0   0.032275   \n",
       "\n",
       "    fit_word2vec  fit_fasttext  \n",
       "94      0.125069      0.000000  \n",
       "53      0.125069      0.131224  \n",
       "62      0.125069      0.131224  \n",
       "40      0.125069      0.131224  \n",
       "10      0.125069      0.131224  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sort_values('fit', ascending=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['fit']==0].job_title.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>fit</th>\n",
       "      <th>fit_tfidf</th>\n",
       "      <th>fit_keras_tokenizer</th>\n",
       "      <th>fit_glove</th>\n",
       "      <th>fit_word2vec</th>\n",
       "      <th>fit_fasttext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>104.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>104.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>52.500000</td>\n",
       "      <td>2.656578</td>\n",
       "      <td>0.328938</td>\n",
       "      <td>0.541808</td>\n",
       "      <td>0.604935</td>\n",
       "      <td>0.594117</td>\n",
       "      <td>0.586781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>30.166206</td>\n",
       "      <td>1.492243</td>\n",
       "      <td>0.315271</td>\n",
       "      <td>0.359278</td>\n",
       "      <td>0.313345</td>\n",
       "      <td>0.322589</td>\n",
       "      <td>0.290476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.243059</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>26.750000</td>\n",
       "      <td>1.585619</td>\n",
       "      <td>0.057644</td>\n",
       "      <td>0.106066</td>\n",
       "      <td>0.286359</td>\n",
       "      <td>0.282529</td>\n",
       "      <td>0.422400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>52.500000</td>\n",
       "      <td>2.951848</td>\n",
       "      <td>0.253802</td>\n",
       "      <td>0.642826</td>\n",
       "      <td>0.664575</td>\n",
       "      <td>0.720907</td>\n",
       "      <td>0.684321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>78.250000</td>\n",
       "      <td>3.549718</td>\n",
       "      <td>0.497634</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.864163</td>\n",
       "      <td>0.845231</td>\n",
       "      <td>0.776623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>104.000000</td>\n",
       "      <td>4.821504</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id         fit   fit_tfidf  fit_keras_tokenizer   fit_glove  \\\n",
       "count  104.000000  104.000000  104.000000           104.000000  104.000000   \n",
       "mean    52.500000    2.656578    0.328938             0.541808    0.604935   \n",
       "std     30.166206    1.492243    0.315271             0.359278    0.313345   \n",
       "min      1.000000    0.243059    0.000000             0.000000    0.000000   \n",
       "25%     26.750000    1.585619    0.057644             0.106066    0.286359   \n",
       "50%     52.500000    2.951848    0.253802             0.642826    0.664575   \n",
       "75%     78.250000    3.549718    0.497634             0.800000    0.864163   \n",
       "max    104.000000    4.821504    1.000000             1.000000    1.000000   \n",
       "\n",
       "       fit_word2vec  fit_fasttext  \n",
       "count    104.000000    104.000000  \n",
       "mean       0.594117      0.586781  \n",
       "std        0.322589      0.290476  \n",
       "min        0.000000      0.000000  \n",
       "25%        0.282529      0.422400  \n",
       "50%        0.720907      0.684321  \n",
       "75%        0.845231      0.776623  \n",
       "max        1.000000      1.000000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No candidates with a fitness score of 0 although the min fit score of each of the fit_columns is all 0.\n",
    "\n",
    "Add a filter column 'has_zero_scores' for candidates with at least 1 'zero' fitness score from the fit_columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_zero_scores = []\n",
    "for i, row in data.iterrows():\n",
    "    has_zero_score = 0\n",
    "    for fit in data.iloc[i][fit_columns]:\n",
    "        if fit == 0:\n",
    "            has_zero_score = 1\n",
    "    \n",
    "    has_zero_scores.append(has_zero_score)\n",
    "\n",
    "data['has_zero_scores'] = has_zero_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Native English Teacher at EPIK (English Program in Korea)',\n",
       "       'Advisory Board Member at Celal Bayar University',\n",
       "       'Student at Chapman University',\n",
       "       'Junior MES Engineer| Information Systems',\n",
       "       'RRP Brand Portfolio Executive at JTI (Japan Tobacco International)',\n",
       "       'Information Systems Specialist and Programmer with a love for data and organization.',\n",
       "       'Bachelor of Science in Biology from Victoria University of Wellington',\n",
       "       'Undergraduate Research Assistant at Styczynski Lab',\n",
       "       'Lead Official at Western Illinois University',\n",
       "       'Admissions Representative at Community medical center long beach',\n",
       "       'Student at Westfield State University',\n",
       "       'Student at Indiana University Kokomo - Business Management - Retail Manager at Delphi Hardware and Paint',\n",
       "       'Student', 'Business Intelligence and Analytics at Travelers',\n",
       "       'Always set them up for Success',\n",
       "       'Director Of Administration at Excellence Logging'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['has_zero_scores'] == 1].job_title.unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can drop these values as they indeed seem irrelavant to our keywords, \"Aspiring human resources\" and \"seeking human resources\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_tfidf least fit job title 1: Director Of Administration at Excellence Logging\n",
      "fit_tfidf least fit job title 2: Native English Teacher at EPIK (English Program in Korea)\n",
      "fit_tfidf least fit job title 3: Bachelor of Science in Biology from Victoria University of Wellington\n",
      "fit_tfidf least fit job title 4: Student at Chapman University\n",
      "fit_tfidf least fit job title 5: Advisory Board Member at Celal Bayar University\n",
      "\n",
      "fit_keras_tokenizer least fit job title 1: Director Of Administration at Excellence Logging\n",
      "fit_keras_tokenizer least fit job title 2: Native English Teacher at EPIK (English Program in Korea)\n",
      "fit_keras_tokenizer least fit job title 3: Advisory Board Member at Celal Bayar University\n",
      "fit_keras_tokenizer least fit job title 4: Native English Teacher at EPIK (English Program in Korea)\n",
      "fit_keras_tokenizer least fit job title 5: Advisory Board Member at Celal Bayar University\n",
      "\n",
      "fit_glove least fit job title 1: Advisory Board Member at Celal Bayar University\n",
      "fit_glove least fit job title 2: Advisory Board Member at Celal Bayar University\n",
      "fit_glove least fit job title 3: Advisory Board Member at Celal Bayar University\n",
      "fit_glove least fit job title 4: Advisory Board Member at Celal Bayar University\n",
      "fit_glove least fit job title 5: Student at Chapman University\n",
      "\n",
      "fit_word2vec least fit job title 1: Native English Teacher at EPIK (English Program in Korea)\n",
      "fit_word2vec least fit job title 2: Native English Teacher at EPIK (English Program in Korea)\n",
      "fit_word2vec least fit job title 3: Native English Teacher at EPIK (English Program in Korea)\n",
      "fit_word2vec least fit job title 4: Native English Teacher at EPIK (English Program in Korea)\n",
      "fit_word2vec least fit job title 5: Native English Teacher at EPIK (English Program in Korea)\n",
      "\n",
      "fit_fasttext least fit job title 1: Student at Westfield State University\n",
      "fit_fasttext least fit job title 2: Always set them up for Success\n",
      "fit_fasttext least fit job title 3: Director Of Administration at Excellence Logging\n",
      "fit_fasttext least fit job title 4: Junior MES Engineer| Information Systems\n",
      "fit_fasttext least fit job title 5: Business Intelligence and Analytics at Travelers\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fit_col in fit_columns:\n",
    "    for i, job_title in enumerate(data.sort_values(fit_col).head().job_title.values):\n",
    "        print(f\"{fit_col} least fit job title {i+1}: {job_title}\")\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the job titles with the worst fitness scores, each evaluation methods for fitness seems to perform fine - i.e. all those 'worst' job titles do not seem relevant to our keywords. In other words, it would be safe to discard candidates with zero fitness scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered = data[ data['has_zero_scores'] != 1 ].drop('has_zero_scores', axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are top 20 'best-fit' candidates and their job titles, after dropping candidates with at least 1 zero fitness scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique job titles of top 20 candidates:\n",
      "['Seeking Human Resources Opportunities'\n",
      " 'Seeking Human Resources Position'\n",
      " 'Aspiring Human Resources Professional'\n",
      " 'Aspiring Human Resources Manager, seeking internship in Human Resources.'\n",
      " 'Aspiring Human Resources Specialist'\n",
      " 'Seeking Human Resources Human Resources Information System and Generalist Positions']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>connection</th>\n",
       "      <th>fit</th>\n",
       "      <th>fit_tfidf</th>\n",
       "      <th>fit_keras_tokenizer</th>\n",
       "      <th>fit_glove</th>\n",
       "      <th>fit_word2vec</th>\n",
       "      <th>fit_fasttext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>Seeking Human Resources Opportunities</td>\n",
       "      <td>Chicago, Illinois</td>\n",
       "      <td>390</td>\n",
       "      <td>4.821504</td>\n",
       "      <td>0.921024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966611</td>\n",
       "      <td>0.933869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>Seeking Human Resources Opportunities</td>\n",
       "      <td>Chicago, Illinois</td>\n",
       "      <td>390</td>\n",
       "      <td>4.821504</td>\n",
       "      <td>0.921024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966611</td>\n",
       "      <td>0.933869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99</td>\n",
       "      <td>Seeking Human Resources Position</td>\n",
       "      <td>Las Vegas, Nevada Area</td>\n",
       "      <td>48</td>\n",
       "      <td>4.755860</td>\n",
       "      <td>0.913385</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.953443</td>\n",
       "      <td>0.966611</td>\n",
       "      <td>0.922421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Aspiring Human Resources Professional</td>\n",
       "      <td>Raleigh-Durham, North Carolina Area</td>\n",
       "      <td>44</td>\n",
       "      <td>4.752130</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.920193</td>\n",
       "      <td>0.874349</td>\n",
       "      <td>0.957589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>Aspiring Human Resources Professional</td>\n",
       "      <td>Raleigh-Durham, North Carolina Area</td>\n",
       "      <td>44</td>\n",
       "      <td>4.752130</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.920193</td>\n",
       "      <td>0.874349</td>\n",
       "      <td>0.957589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21</td>\n",
       "      <td>Aspiring Human Resources Professional</td>\n",
       "      <td>Raleigh-Durham, North Carolina Area</td>\n",
       "      <td>44</td>\n",
       "      <td>4.752130</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.920193</td>\n",
       "      <td>0.874349</td>\n",
       "      <td>0.957589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>33</td>\n",
       "      <td>Aspiring Human Resources Professional</td>\n",
       "      <td>Raleigh-Durham, North Carolina Area</td>\n",
       "      <td>44</td>\n",
       "      <td>4.752130</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.920193</td>\n",
       "      <td>0.874349</td>\n",
       "      <td>0.957589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>46</td>\n",
       "      <td>Aspiring Human Resources Professional</td>\n",
       "      <td>Raleigh-Durham, North Carolina Area</td>\n",
       "      <td>44</td>\n",
       "      <td>4.752130</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.920193</td>\n",
       "      <td>0.874349</td>\n",
       "      <td>0.957589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>58</td>\n",
       "      <td>Aspiring Human Resources Professional</td>\n",
       "      <td>Raleigh-Durham, North Carolina Area</td>\n",
       "      <td>44</td>\n",
       "      <td>4.752130</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.920193</td>\n",
       "      <td>0.874349</td>\n",
       "      <td>0.957589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>97</td>\n",
       "      <td>Aspiring Human Resources Professional</td>\n",
       "      <td>Kokomo, Indiana Area</td>\n",
       "      <td>71</td>\n",
       "      <td>4.752130</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.920193</td>\n",
       "      <td>0.874349</td>\n",
       "      <td>0.957589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>73</td>\n",
       "      <td>Aspiring Human Resources Manager, seeking inte...</td>\n",
       "      <td>Houston, Texas Area</td>\n",
       "      <td>7</td>\n",
       "      <td>4.609975</td>\n",
       "      <td>0.648168</td>\n",
       "      <td>0.979796</td>\n",
       "      <td>0.982011</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6</td>\n",
       "      <td>Aspiring Human Resources Specialist</td>\n",
       "      <td>Greater New York City Area</td>\n",
       "      <td>1</td>\n",
       "      <td>4.548911</td>\n",
       "      <td>0.830646</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962413</td>\n",
       "      <td>0.842461</td>\n",
       "      <td>0.913391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>24</td>\n",
       "      <td>Aspiring Human Resources Specialist</td>\n",
       "      <td>Greater New York City Area</td>\n",
       "      <td>1</td>\n",
       "      <td>4.548911</td>\n",
       "      <td>0.830646</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962413</td>\n",
       "      <td>0.842461</td>\n",
       "      <td>0.913391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>36</td>\n",
       "      <td>Aspiring Human Resources Specialist</td>\n",
       "      <td>Greater New York City Area</td>\n",
       "      <td>1</td>\n",
       "      <td>4.548911</td>\n",
       "      <td>0.830646</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962413</td>\n",
       "      <td>0.842461</td>\n",
       "      <td>0.913391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>49</td>\n",
       "      <td>Aspiring Human Resources Specialist</td>\n",
       "      <td>Greater New York City Area</td>\n",
       "      <td>1</td>\n",
       "      <td>4.548911</td>\n",
       "      <td>0.830646</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962413</td>\n",
       "      <td>0.842461</td>\n",
       "      <td>0.913391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>60</td>\n",
       "      <td>Aspiring Human Resources Specialist</td>\n",
       "      <td>Greater New York City Area</td>\n",
       "      <td>1</td>\n",
       "      <td>4.548911</td>\n",
       "      <td>0.830646</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962413</td>\n",
       "      <td>0.842461</td>\n",
       "      <td>0.913391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10</td>\n",
       "      <td>Seeking Human Resources Human Resources Inform...</td>\n",
       "      <td>Greater Philadelphia Area</td>\n",
       "      <td>500+</td>\n",
       "      <td>4.282827</td>\n",
       "      <td>0.736714</td>\n",
       "      <td>0.755929</td>\n",
       "      <td>0.953667</td>\n",
       "      <td>0.926198</td>\n",
       "      <td>0.910319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>40</td>\n",
       "      <td>Seeking Human Resources Human Resources Inform...</td>\n",
       "      <td>Greater Philadelphia Area</td>\n",
       "      <td>500+</td>\n",
       "      <td>4.282827</td>\n",
       "      <td>0.736714</td>\n",
       "      <td>0.755929</td>\n",
       "      <td>0.953667</td>\n",
       "      <td>0.926198</td>\n",
       "      <td>0.910319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>53</td>\n",
       "      <td>Seeking Human Resources Human Resources Inform...</td>\n",
       "      <td>Greater Philadelphia Area</td>\n",
       "      <td>500+</td>\n",
       "      <td>4.282827</td>\n",
       "      <td>0.736714</td>\n",
       "      <td>0.755929</td>\n",
       "      <td>0.953667</td>\n",
       "      <td>0.926198</td>\n",
       "      <td>0.910319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>62</td>\n",
       "      <td>Seeking Human Resources Human Resources Inform...</td>\n",
       "      <td>Greater Philadelphia Area</td>\n",
       "      <td>500+</td>\n",
       "      <td>4.282827</td>\n",
       "      <td>0.736714</td>\n",
       "      <td>0.755929</td>\n",
       "      <td>0.953667</td>\n",
       "      <td>0.926198</td>\n",
       "      <td>0.910319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                          job_title  \\\n",
       "0   28              Seeking Human Resources Opportunities   \n",
       "1   30              Seeking Human Resources Opportunities   \n",
       "2   99                   Seeking Human Resources Position   \n",
       "3    3              Aspiring Human Resources Professional   \n",
       "4   17              Aspiring Human Resources Professional   \n",
       "5   21              Aspiring Human Resources Professional   \n",
       "6   33              Aspiring Human Resources Professional   \n",
       "7   46              Aspiring Human Resources Professional   \n",
       "8   58              Aspiring Human Resources Professional   \n",
       "9   97              Aspiring Human Resources Professional   \n",
       "10  73  Aspiring Human Resources Manager, seeking inte...   \n",
       "11   6                Aspiring Human Resources Specialist   \n",
       "12  24                Aspiring Human Resources Specialist   \n",
       "13  36                Aspiring Human Resources Specialist   \n",
       "14  49                Aspiring Human Resources Specialist   \n",
       "15  60                Aspiring Human Resources Specialist   \n",
       "16  10  Seeking Human Resources Human Resources Inform...   \n",
       "17  40  Seeking Human Resources Human Resources Inform...   \n",
       "18  53  Seeking Human Resources Human Resources Inform...   \n",
       "19  62  Seeking Human Resources Human Resources Inform...   \n",
       "\n",
       "                               location connection       fit  fit_tfidf  \\\n",
       "0                     Chicago, Illinois        390  4.821504   0.921024   \n",
       "1                     Chicago, Illinois        390  4.821504   0.921024   \n",
       "2                Las Vegas, Nevada Area         48  4.755860   0.913385   \n",
       "3   Raleigh-Durham, North Carolina Area         44  4.752130   1.000000   \n",
       "4   Raleigh-Durham, North Carolina Area         44  4.752130   1.000000   \n",
       "5   Raleigh-Durham, North Carolina Area         44  4.752130   1.000000   \n",
       "6   Raleigh-Durham, North Carolina Area         44  4.752130   1.000000   \n",
       "7   Raleigh-Durham, North Carolina Area         44  4.752130   1.000000   \n",
       "8   Raleigh-Durham, North Carolina Area         44  4.752130   1.000000   \n",
       "9                  Kokomo, Indiana Area         71  4.752130   1.000000   \n",
       "10                  Houston, Texas Area          7  4.609975   0.648168   \n",
       "11           Greater New York City Area          1  4.548911   0.830646   \n",
       "12           Greater New York City Area          1  4.548911   0.830646   \n",
       "13           Greater New York City Area          1  4.548911   0.830646   \n",
       "14           Greater New York City Area          1  4.548911   0.830646   \n",
       "15           Greater New York City Area          1  4.548911   0.830646   \n",
       "16            Greater Philadelphia Area      500+   4.282827   0.736714   \n",
       "17            Greater Philadelphia Area      500+   4.282827   0.736714   \n",
       "18            Greater Philadelphia Area      500+   4.282827   0.736714   \n",
       "19            Greater Philadelphia Area      500+   4.282827   0.736714   \n",
       "\n",
       "    fit_keras_tokenizer  fit_glove  fit_word2vec  fit_fasttext  \n",
       "0              1.000000   1.000000      0.966611      0.933869  \n",
       "1              1.000000   1.000000      0.966611      0.933869  \n",
       "2              1.000000   0.953443      0.966611      0.922421  \n",
       "3              1.000000   0.920193      0.874349      0.957589  \n",
       "4              1.000000   0.920193      0.874349      0.957589  \n",
       "5              1.000000   0.920193      0.874349      0.957589  \n",
       "6              1.000000   0.920193      0.874349      0.957589  \n",
       "7              1.000000   0.920193      0.874349      0.957589  \n",
       "8              1.000000   0.920193      0.874349      0.957589  \n",
       "9              1.000000   0.920193      0.874349      0.957589  \n",
       "10             0.979796   0.982011      1.000000      1.000000  \n",
       "11             1.000000   0.962413      0.842461      0.913391  \n",
       "12             1.000000   0.962413      0.842461      0.913391  \n",
       "13             1.000000   0.962413      0.842461      0.913391  \n",
       "14             1.000000   0.962413      0.842461      0.913391  \n",
       "15             1.000000   0.962413      0.842461      0.913391  \n",
       "16             0.755929   0.953667      0.926198      0.910319  \n",
       "17             0.755929   0.953667      0.926198      0.910319  \n",
       "18             0.755929   0.953667      0.926198      0.910319  \n",
       "19             0.755929   0.953667      0.926198      0.910319  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filtered = data_filtered.sort_values(['fit', 'id', 'connection'], ascending=[False, True, True]).reset_index(drop=True)\n",
    "print(f\"Unique job titles of top 20 candidates:\\n{data_filtered.head(20).job_title.unique()}\")\n",
    "data_filtered.head(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train ranking models - XGBoost and LGBM Rankers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize job titles using fasttext and use the word vectors as training features.<br>\n",
    "Set the 'id' column as index, and the fitness score of each candidate under the 'fit' column will be the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features = pd.DataFrame(\n",
    "    get_word_vectors(data_filtered, 'job_title', vectorizer='fasttext',\n",
    "                     to_process_text=True, remove_stopwords=True, lemmatize=False, stem=False)\n",
    ")\n",
    "data_selected = pd.concat([data_filtered, training_features], axis=1).set_index('id')\n",
    "X = data_selected[training_features.columns]\n",
    "y = data_selected['fit']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train and test sets before train any model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "random_state = 1\n",
    "X_train, X_test, y_train_fitness, y_test_fitness = split_data(\n",
    "    X, y, test_size, random_state=random_state, oversampling=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert fitness scores into ranks for y_train and y_test separately so that each data set has ranks starting from 1 to the number of data points. Also change the name of the series (or column) from 'fit' to 'rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train_fitness.rank(method='dense', ascending=False)\n",
    "y_train.name = 'rank'\n",
    "\n",
    "y_test = y_test_fitness.rank(method='dense', ascending=False)\n",
    "y_test.name = 'rank'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1) Train an XGBoost Ranker, and get prediction results.\n",
    "\n",
    "descriptions about XGBoost to be added.\n",
    "* Evaluation metric: NDCG (normalized discounted cumulative gain) is a measure of the effectiveness of a ranking system, taking into account the position of relevant items in the ranked list. It is based on the idea that items that are higher in the ranking should be given more credit than items that are lower in the ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth stats:\n",
      "                      y_train  y_test\n",
      "Mean (Top 5 rankers)   2.7368  3.3750\n",
      "Mean                  15.0000  6.1875\n",
      "Std                   10.4865  3.3310 \n",
      "\n",
      "Train stats:\n",
      "Mean rank of top 5 candidates based on predictions: 5.6316\n",
      "Mean rank of all candidates based on predictions: 15.5\n",
      "Std rank of all candidates based on predictions: 9.7354\n",
      "Mean absolute difference between each pair of rank and predicted rank: 2.9194\n",
      "    rank  pred_rank  abs_diff\n",
      "id                           \n",
      "27   6.0        1.0       5.0\n",
      "29   6.0        1.0       5.0\n",
      "28   1.0        2.0       1.0\n",
      "30   1.0        2.0       1.0\n",
      "60   3.0        3.0       0.0 \n",
      "\n",
      "Test stats:\n",
      "Mean rank of top 5 candidates based on predictions: 2.875\n",
      "Mean rank of all candidates based on predictions: 5.9375\n",
      "Std rank of all candidates based on predictions: 3.8204\n",
      "Mean absolute difference between each pair of rank and predicted rank: 1.875\n",
      "    rank  pred_rank  abs_diff\n",
      "id                           \n",
      "99   1.0        1.0       0.0\n",
      "25   4.0        2.0       2.0\n",
      "52   4.0        2.0       2.0\n",
      "9    4.0        2.0       2.0\n",
      "39   4.0        2.0       2.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_params = {\n",
    "    # 'n_estimators': 40,\n",
    "    # 'max_depth': 2,\n",
    "    # 'learning_rate': 0.02, # same as xgb's eta; default=0.3\n",
    "    'objective': 'rank:pairwise', # perform better than 'rank:ndcg'\n",
    "    'booster': 'gbtree',\n",
    "    'eval_metric': 'ndcg',\n",
    "    # 'subsample': 0.5,\n",
    "    # 'gamma': 4.5, # default=0; the larger the more conservative\n",
    "    # 'min_child_weight': 2, # default=1; the larger the more conservative\n",
    "    'random_state': random_state\n",
    "}\n",
    "\n",
    "xgb_ranker = XGBRanker(**xgb_params)\n",
    "xgb_ranker.fit(X_train, y_train,\n",
    "               group=y_train.value_counts(),\n",
    "               eval_set=[(X_test, y_test)],\n",
    "               eval_group=[y_test.value_counts()]\n",
    "               )\n",
    "\n",
    "stats_df = pd.DataFrame(\n",
    "    index=[\"Mean (Top 5 rankers)\", \"Mean\", \"Std\"],\n",
    "    columns=[\"y_train\", \"y_test\"],\n",
    "    data=[\n",
    "        [round(y_train[y_train<=5].mean(), 4),\n",
    "         round(y_test[y_test<=5].mean(), 4)],\n",
    "        [round(y_train.mean(), 4),\n",
    "         round(y_test.mean(), 4)],\n",
    "        [round(y_train.std(), 4),\n",
    "         round(y_test.std(), 4)]\n",
    "    ]\n",
    ")\n",
    "print(\"Ground truth stats:\")\n",
    "print(stats_df,\"\\n\")\n",
    "\n",
    "print(\"Train stats:\")\n",
    "xgb_train_result = get_rank_predictions(X_train, y_train, xgb_ranker, target_column=\"rank\", target=\"candidates\")\n",
    "\n",
    "print(\"Test stats:\")\n",
    "xgb_test_result = get_rank_predictions(X_test, y_test, xgb_ranker, target_column=\"rank\", target=\"candidates\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Train an LGBM (Light Gradient-Boosting Machine) Ranker, and get prediction results.\n",
    "\n",
    "descriptions about LGBM to be added.\n",
    "* Ranking algorithm: LambdaRank. This is a technique where ranking is transformed into a pairwise classification or regression problem. Basically, the algorithms consider a pair of items at a single time to come up with a viable ordering of those items before initiating the final order of the entire list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth stats:\n",
      "                         y_train  y_test\n",
      "0  Mean (Top 5 rankers)   2.7368  3.3750\n",
      "1                  Mean  15.0000  6.1875\n",
      "2                   Std  10.4865  3.3310 \n",
      "\n",
      "Train stats:\n",
      "Mean rank of top 5 candidates based on predictions: 4.7368\n",
      "Mean rank of all candidates based on predictions: 15.2258\n",
      "Std rank of all candidates based on predictions: 10.2436\n",
      "Mean absolute difference between each pair of rank and predicted rank: 2.6129\n",
      "    rank  pred_rank  abs_diff\n",
      "id                           \n",
      "60   3.0        1.0       2.0\n",
      "36   3.0        1.0       2.0\n",
      "49   3.0        1.0       2.0\n",
      "6    3.0        1.0       2.0\n",
      "24   3.0        1.0       2.0 \n",
      "\n",
      "Test stats:\n",
      "Mean rank of top 5 candidates based on predictions: 4.875\n",
      "Mean rank of all candidates based on predictions: 6.8125\n",
      "Std rank of all candidates based on predictions: 3.2087\n",
      "Mean absolute difference between each pair of rank and predicted rank: 2.0\n",
      "    rank  pred_rank  abs_diff\n",
      "id                           \n",
      "66   3.0        1.0       2.0\n",
      "99   1.0        2.0       1.0\n",
      "73   2.0        3.0       1.0\n",
      "83   8.0        4.0       4.0\n",
      "61   5.0        5.0       0.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lgbm_ranker = LGBMRanker(\n",
    "    boosting_type=\"dart\", # 'gbdt', 'dart', 'rf'\n",
    "    # max_depth=2,\n",
    "    objective=\"lambdarank\",\n",
    "    metric= \"ndcg\",\n",
    "    label_gain =[i for i in range(int(max(y_train.max(), y_test.max())) + 2)],\n",
    "    random_state=random_state\n",
    "    )\n",
    "\n",
    "lgbm_ranker.fit(\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    group=y_train.value_counts(),\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    eval_group=[y_test.value_counts()],\n",
    "    verbose=-1\n",
    "    )\n",
    "\n",
    "print(\"Ground truth stats:\")\n",
    "print(stats_df,\"\\n\")\n",
    "\n",
    "print(\"Train stats:\")\n",
    "lgbm_train_result = get_rank_predictions(X_train, y_train, lgbm_ranker, target_column=\"rank\", target=\"candidates\")\n",
    "\n",
    "print(\"Test stats:\")\n",
    "lgbm_test_result = get_rank_predictions(X_test, y_test, lgbm_ranker, target_column=\"rank\", target=\"candidates\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both rankers successfully predicted the top-ranked candidates (i.e. pred_rank == 1.0) as they all were selected candidates.\n",
    "\n",
    "In terms of the mean of predicted ranks for selected candidates, the LGBM ranker performed better at predicting selected candidates. Therefore, the model will be used instead of the XGB ranker for the rest of the project.\n",
    "\n",
    "* Overall, the predictions by the LGBM ranker were quite close to the ground truths without much hyperparameter tuning, even though some hyperparameter tuning was done for the XGB ranker."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Star ideal candidates and re-train the ranking model based on the updated ranks/criteria.\n",
    "\n",
    "Since we've built our base ranking model, proceed to starring ideal candidates and re-rank all candidates based on the stars.\n",
    "* Starring one candidate sets this candidate as an ideal candidate for the given role. Then, we re-rank the list each time a candidate or a list of candidates is starred."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5-1. Take user input for the ids of ideal candidates (i.e. the candidates to star)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = input(f\"\"\"\n",
    "# Enter IDs of ideal candidates, separated by comma:\n",
    "# * IDs: {unique_ids}\n",
    "# \"\"\")\n",
    "input_ids = \"1, 2, 5, 10, 12\"\n",
    "ideal_candidates = sorted([int(id.strip()) for id in input_ids.split(',')])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5-2. Create a copy of the train and test data to rerank candidates based on starring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_updated = X_train.copy()\n",
    "X_test_updated = X_test.copy()\n",
    "y_train_updated = y_train.copy()\n",
    "y_test_updated = y_test.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5-3. Add a binary feature 'star' to X_train and X_test.\n",
    "\n",
    "If a candidate is 'starred', the feature value will be 1, otherwise 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_updated['star'] = [1 if idx in ideal_candidates else 0 for idx in X_train_updated.index]\n",
    "X_test_updated['star'] = [1 if idx in ideal_candidates else 0 for idx in X_test_updated.index]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5-4. Add 1 to the ranks of all candidates (e.g. rank 1 will become rank 2) so that the starred candidates can become the top ranker with a rank of 1.\n",
    "\n",
    "Update y_train and y_test with the updated ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank of candidate 1 in y_train updated to 1.\n",
      "Candidate 2 not found!\n",
      "Candidate 5 not found!\n",
      "Rank of candidate 10 in y_train updated to 1.\n",
      "Rank of candidate 12 in y_test updated to 1.\n"
     ]
    }
   ],
   "source": [
    "y_train_updated += 1\n",
    "y_test_updated += 1\n",
    "\n",
    "ideal_rank = 1\n",
    "for id in ideal_candidates:\n",
    "    if id in y_train_updated.index:\n",
    "        y_train_updated[id] = ideal_rank\n",
    "        print(f\"Rank of candidate {id} in y_train updated to {ideal_rank}.\")\n",
    "    elif id in y_test.index:\n",
    "        y_test_updated[id] = ideal_rank\n",
    "        print(f\"Rank of candidate {id} in y_test updated to {ideal_rank}.\")\n",
    "    else:\n",
    "        print(f\"Candidate {id} not found!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5-5. Re-train ranking models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Updated) Ground truth stats:\n",
      "                      y_train_updated  y_test_updated\n",
      "Mean (Top 5 rankers)           3.2632          3.7500\n",
      "Mean                          15.4677          6.8125\n",
      "Std                           10.5764          3.6737 \n",
      "\n",
      "(Updated) Train stats:\n",
      "Mean rank of top 5 candidates based on predictions: 5.0\n",
      "Mean rank of all candidates based on predictions: 15.2581\n",
      "Std rank of all candidates based on predictions: 10.34\n",
      "Mean absolute difference between each pair of rank and predicted rank: 2.8226\n",
      "    rank  pred_rank  abs_diff\n",
      "id                           \n",
      "60   4.0        1.0       3.0\n",
      "36   4.0        1.0       3.0\n",
      "49   4.0        1.0       3.0\n",
      "6    4.0        1.0       3.0\n",
      "24   4.0        1.0       3.0 \n",
      "\n",
      "(Updated) Test stats:\n",
      "Mean rank of top 5 candidates based on predictions: 3.125\n",
      "Mean rank of all candidates based on predictions: 5.875\n",
      "Std rank of all candidates based on predictions: 3.7394\n",
      "Mean absolute difference between each pair of rank and predicted rank: 1.9375\n",
      "    rank  pred_rank  abs_diff\n",
      "id                           \n",
      "99   2.0        1.0       1.0\n",
      "25   5.0        2.0       3.0\n",
      "52   5.0        2.0       3.0\n",
      "9    5.0        2.0       3.0\n",
      "39   5.0        2.0       3.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_ranker.fit(X_train_updated, y_train_updated,\n",
    "               group=y_train_updated.value_counts(),\n",
    "               eval_set=[(X_test_updated, y_test_updated)],\n",
    "               eval_group=[y_test_updated.value_counts()]\n",
    "               )\n",
    "\n",
    "stats_df_updated = pd.DataFrame(\n",
    "    index=[\"Mean (Top 5 rankers)\", \"Mean\", \"Std\"],\n",
    "    columns=[\"y_train_updated\", \"y_test_updated\"],\n",
    "    data=[\n",
    "        [round(y_train_updated[y_train_updated<=5].mean(), 4),\n",
    "         round(y_test_updated[y_test_updated<=5].mean(), 4)],\n",
    "        [round(y_train_updated.mean(), 4),\n",
    "         round(y_test_updated.mean(), 4)],\n",
    "        [round(y_train_updated.std(), 4),\n",
    "         round(y_test_updated.std(), 4)]\n",
    "    ]\n",
    ")\n",
    "print(\"(Updated) Ground truth stats:\")\n",
    "print(stats_df_updated,\"\\n\")\n",
    "\n",
    "print(\"(Updated) Train stats:\")\n",
    "xgb_train_result_updated = get_rank_predictions(\n",
    "    X_train_updated, y_train_updated, xgb_ranker, target_column=\"rank\", target=\"candidates\")\n",
    "\n",
    "print(\"(Updated) Test stats:\")\n",
    "xgb_test_result_updated = get_rank_predictions(\n",
    "    X_test_updated, y_test_updated, xgb_ranker, target_column=\"rank\", target=\"candidates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Updated) Ground truth stats:\n",
      "                         y_train_updated  y_test_updated\n",
      "0  Mean (Top 5 rankers)           3.2632          3.7500\n",
      "1                  Mean          15.4677          6.8125\n",
      "2                   Std          10.5764          3.6737 \n",
      "\n",
      "(Updated) Train stats:\n",
      "Mean rank of top 5 candidates based on predictions: 3.8947\n",
      "Mean rank of all candidates based on predictions: 14.9839\n",
      "Std rank of all candidates based on predictions: 10.6916\n",
      "Mean absolute difference between each pair of rank and predicted rank: 2.6774\n",
      "    rank  pred_rank  abs_diff\n",
      "id                           \n",
      "3    3.0        1.0       2.0\n",
      "58   3.0        1.0       2.0\n",
      "46   3.0        1.0       2.0\n",
      "17   3.0        1.0       2.0\n",
      "33   3.0        1.0       2.0 \n",
      "\n",
      "(Updated) Test stats:\n",
      "Mean rank of top 5 candidates based on predictions: 5.0\n",
      "Mean rank of all candidates based on predictions: 6.5625\n",
      "Std rank of all candidates based on predictions: 3.1616\n",
      "Mean absolute difference between each pair of rank and predicted rank: 1.875\n",
      "    rank  pred_rank  abs_diff\n",
      "id                           \n",
      "99   2.0        1.0       1.0\n",
      "66   4.0        2.0       2.0\n",
      "73   3.0        3.0       0.0\n",
      "83   9.0        4.0       5.0\n",
      "61   6.0        5.0       1.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lgbm_ranker.fit(\n",
    "    X=X_train_updated,\n",
    "    y=y_train_updated,\n",
    "    group=y_train_updated.value_counts(),\n",
    "    eval_set=[(X_test_updated, y_test_updated)],\n",
    "    eval_group=[y_test_updated.value_counts()],\n",
    "    verbose=-1\n",
    "    )\n",
    "\n",
    "print(\"(Updated) Ground truth stats:\")\n",
    "print(stats_df_updated,\"\\n\")\n",
    "\n",
    "print(\"(Updated) Train stats:\")\n",
    "lgbm_train_result_updated = get_rank_predictions(\n",
    "    X_train_updated, y_train_updated, lgbm_ranker, target_column=\"rank\", target=\"candidates\")\n",
    "\n",
    "print(\"(Updated) Test stats:\")\n",
    "lgbm_test_result_updated = get_rank_predictions(\n",
    "    X_test_updated, y_test_updated, lgbm_ranker, target_column=\"rank\", target=\"candidates\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5-4. Evaluate results.\n",
    "\n",
    "Collate and re-arrange statistics for easier evaluation of the results and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_train</th>\n",
       "      <th>y_train_updated</th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_test_updated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mean (Top 5 rankers)</th>\n",
       "      <td>2.7368</td>\n",
       "      <td>3.2632</td>\n",
       "      <td>3.3750</td>\n",
       "      <td>3.7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean</th>\n",
       "      <td>15.0000</td>\n",
       "      <td>15.4677</td>\n",
       "      <td>6.1875</td>\n",
       "      <td>6.8125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Std</th>\n",
       "      <td>10.4865</td>\n",
       "      <td>10.5764</td>\n",
       "      <td>3.3310</td>\n",
       "      <td>3.6737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      y_train  y_train_updated  y_test  y_test_updated\n",
       "Mean (Top 5 rankers)   2.7368           3.2632  3.3750          3.7500\n",
       "Mean                  15.0000          15.4677  6.1875          6.8125\n",
       "Std                   10.4865          10.5764  3.3310          3.6737"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_df_concat = pd.concat([stats_df, stats_df_updated], axis=1)\n",
    "stats_df_concat = stats_df_concat.iloc[:, [0, 2, 1, 3]] # re-arrange columns\n",
    "stats_df_concat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions on top 5 candidates were similar before and after the starring of ideal candidates. In other words, the ranking models can handle the update of ranks based on stars, i.e. the new binary column 'star'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall mean statistics:\n",
      "           xgb_train  xgb_train_updated  xgb_test  xgb_test_updated  \\\n",
      "rank         15.0000            15.4677    6.1875            6.8125   \n",
      "pred_rank    15.5000            15.2581    5.9375            5.8750   \n",
      "abs_diff      2.9194             2.8226    1.8750            1.9375   \n",
      "\n",
      "           lgbm_train  lgbm_train_updated  lgbm_test  lgbm_test_updated  \n",
      "rank          15.0000             15.4677     6.1875             6.8125  \n",
      "pred_rank     15.2258             14.9839     6.8125             6.5625  \n",
      "abs_diff       2.6129              2.6774     2.0000             1.8750  \n"
     ]
    }
   ],
   "source": [
    "overall_mean_dict = {}\n",
    "\n",
    "overall_mean_dict[\"xgb_train\"] = round(xgb_train_result.mean(), 4).to_dict()\n",
    "overall_mean_dict[\"xgb_train_updated\"] = round(xgb_train_result_updated.mean(), 4).to_dict()\n",
    "overall_mean_dict[\"xgb_test\"] = round(xgb_test_result.mean(), 4).to_dict()\n",
    "overall_mean_dict[\"xgb_test_updated\"] = round(xgb_test_result_updated.mean(), 4).to_dict()\n",
    "\n",
    "overall_mean_dict[\"lgbm_train\"] = round(lgbm_train_result.mean(), 4).to_dict()\n",
    "overall_mean_dict[\"lgbm_train_updated\"] = round(lgbm_train_result_updated.mean(), 4).to_dict()\n",
    "overall_mean_dict[\"lgbm_test\"] = round(lgbm_test_result.mean(), 4).to_dict()\n",
    "overall_mean_dict[\"lgbm_test_updated\"] = round(lgbm_test_result_updated.mean(), 4).to_dict()\n",
    "\n",
    "overall_mean_df = pd.DataFrame(overall_mean_dict)\n",
    "print(f\"Overall mean statistics:\\n{overall_mean_df}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the mean absolute difference between the real ranks and predicted ranks did not change much after re-training of the models based on the starring of ideal candidates.\n",
    "\n",
    "Based on these statistics, the model performance of the two rankers do not seem significantly different, although the LGBM ranker's predictions were slightly better than the XGB ranker's predictions - i.e. smaller abs_diffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 mean statistics:\n",
      "           xgb_train  xgb_train_updated  xgb_test  xgb_test_updated  \\\n",
      "rank          2.7368             3.2632     3.375             3.750   \n",
      "pred_rank     5.6316             5.0000     2.875             3.125   \n",
      "abs_diff      2.8947             3.3158     1.500             2.625   \n",
      "\n",
      "           lgbm_train  lgbm_train_updated  lgbm_test  lgbm_test_updated  \n",
      "rank           2.7368              3.2632      3.375               3.75  \n",
      "pred_rank      4.7368              3.8947      4.875               5.00  \n",
      "abs_diff       3.0526              3.1579      2.000               2.00  \n"
     ]
    }
   ],
   "source": [
    "top5_mean_dict = {}\n",
    "\n",
    "top5_mean_dict[\"xgb_train\"] = round(xgb_train_result[xgb_train_result[\"rank\"]<=5].mean(), 4).to_dict()\n",
    "top5_mean_dict[\"xgb_train_updated\"] = round(xgb_train_result_updated[xgb_train_result_updated[\"rank\"]<=5].mean(), 4).to_dict()\n",
    "top5_mean_dict[\"xgb_test\"] = round(xgb_test_result[xgb_test_result[\"rank\"]<=5].mean(), 4).to_dict()\n",
    "top5_mean_dict[\"xgb_test_updated\"] = round(xgb_test_result_updated[xgb_test_result_updated[\"rank\"]<=5].mean(), 4).to_dict()\n",
    "\n",
    "top5_mean_dict[\"lgbm_train\"] = round(lgbm_train_result[lgbm_train_result[\"rank\"]<=5].mean(), 4).to_dict()\n",
    "top5_mean_dict[\"lgbm_train_updated\"] = round(lgbm_train_result_updated[lgbm_train_result_updated[\"rank\"]<=5].mean(), 4).to_dict()\n",
    "top5_mean_dict[\"lgbm_test\"] = round(lgbm_test_result[lgbm_test_result[\"rank\"]<=5].mean(), 4).to_dict()\n",
    "top5_mean_dict[\"lgbm_test_updated\"] = round(lgbm_test_result_updated[lgbm_test_result_updated[\"rank\"]<=5].mean(), 4).to_dict()\n",
    "\n",
    "top5_mean_df = pd.DataFrame(top5_mean_dict)\n",
    "print(f\"Top 5 mean statistics:\\n{top5_mean_df}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we only look at the top 5 rankers (based on the ground truth ranks), the LGBM ranker still performed better.\n",
    "\n",
    "Also worth noting is that the differences between mean values before and after the starring operation (e.g. between xgb_train and xgb_train_updated) are larger for top 5 candidates, whereas the same differences for all candidates were quite small."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- save and load the model for re-training model and making predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a4868653bb6f8972e87e4c446ab8a445a15b25dedb8594cc74c480f8152ea86a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
