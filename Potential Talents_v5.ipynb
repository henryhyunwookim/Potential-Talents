{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.load import load_data\n",
    "from utils.split import split_data\n",
    "from utils.process_text import process_text, convert_terms, convert_words_to_vectors, get_word_vectors\n",
    "from utils.predict import get_rank_predictions\n",
    "from utils.evaluate import cosine_similarity\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "from xgboost import XGBRanker\n",
    "from lightgbm import LGBMRanker\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 104 entries, 0 to 103\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   id          104 non-null    int64  \n",
      " 1   job_title   104 non-null    object \n",
      " 2   location    104 non-null    object \n",
      " 3   connection  104 non-null    object \n",
      " 4   fit         0 non-null      float64\n",
      "dtypes: float64(1), int64(1), object(3)\n",
      "memory usage: 4.2+ KB\n",
      "None \n",
      "\n",
      "               id  fit\n",
      "count  104.000000  0.0\n",
      "mean    52.500000  NaN\n",
      "std     30.166206  NaN\n",
      "min      1.000000  NaN\n",
      "25%     26.750000  NaN\n",
      "50%     52.500000  NaN\n",
      "75%     78.250000  NaN\n",
      "max    104.000000  NaN \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>connection</th>\n",
       "      <th>fit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2019 C.T. Bauer College of Business Graduate (...</td>\n",
       "      <td>Houston, Texas</td>\n",
       "      <td>85</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Native English Teacher at EPIK (English Progra...</td>\n",
       "      <td>Kanada</td>\n",
       "      <td>500+</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Aspiring Human Resources Professional</td>\n",
       "      <td>Raleigh-Durham, North Carolina Area</td>\n",
       "      <td>44</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>People Development Coordinator at Ryan</td>\n",
       "      <td>Denton, Texas</td>\n",
       "      <td>500+</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Advisory Board Member at Celal Bayar University</td>\n",
       "      <td>İzmir, Türkiye</td>\n",
       "      <td>500+</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                          job_title  \\\n",
       "0   1  2019 C.T. Bauer College of Business Graduate (...   \n",
       "1   2  Native English Teacher at EPIK (English Progra...   \n",
       "2   3              Aspiring Human Resources Professional   \n",
       "3   4             People Development Coordinator at Ryan   \n",
       "4   5    Advisory Board Member at Celal Bayar University   \n",
       "\n",
       "                              location connection  fit  \n",
       "0                       Houston, Texas         85  NaN  \n",
       "1                               Kanada      500+   NaN  \n",
       "2  Raleigh-Durham, North Carolina Area         44  NaN  \n",
       "3                        Denton, Texas      500+   NaN  \n",
       "4                       İzmir, Türkiye      500+   NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_data(file_name=\"potential-talents.xlsx\", folder_name=\"data\")\n",
    "print(data.info(), \"\\n\")\n",
    "print(data.describe(), \"\\n\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ProfileReport(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The id column is just an index column that would not be relevant to the fitness of any roles.\n",
    "Although the job_title and location columns are highly correlated, the job_title column seems to be the only relevant column in determining the fitness of a particular role based on the column values and information we have about the requirements.\n",
    "\n",
    "Therefore, only the job_title column will be used in the ranking procedures. Having said that, the other columns will still be returned in the result so that the user (i.e. the client) can have the full information about each of the relevant candidates.\n",
    "The fit column will be filled with a fitness score for each row/candidate later."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pre-process job titles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert human resources-related terms in a way that job titles containing those terms will have better fitness scores. That is, those job titles might end up having a fitness score of 0 without conversion because for instance \"HR\" and \"Human Resources\" would be considered to have nothing in common by most algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GPHR', 'HR', 'HRIS', 'CHRO,', 'SPHR']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_title_words = list(set(\" \".join(data['job_title']).split()))\n",
    "hr_words = [word for word in job_title_words if \"HR\" in word]\n",
    "hr_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_terms_dict = {'CHRO,': 'Chief Human Resources Officer,',\n",
    "                'GPHR': 'Global Professional in Human Resources',\n",
    "                'SPHR': 'Senior Professional in Human Resources',\n",
    "                'HR': 'Human Resources',\n",
    "                'HRIS': 'Human Resources Information System',\n",
    "                'People': 'Human'} # this is for titles like 'People Development Coordinator at Ryan'.\n",
    "\n",
    "for i, job_title in enumerate(data['job_title']):\n",
    "    converted = []\n",
    "    for word in job_title.split():\n",
    "        converted.append(convert_terms(word, hr_terms_dict))\n",
    "    data.loc[i, 'job_title'] = \" \".join(converted)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar conversions can be done for terms like \"staff*\", \"employ*\", but we will leave the decision to domain experts later and for now only convert terms that specifically include \"HR\" as above."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get fitness scores\n",
    "\n",
    "Vectorize job titles and keywords using different techniques and calculate cosine similarity between the vectors - higher cosine similarity would mean the job titles and keywords are more closely related."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-1. TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "TF-IDF quantifies how relevant a word is to a document in a collection of documents or corpus.\n",
    "\n",
    "For instance, if a word appears in a document many times but also appears many times across different documents in the collection, then the word will have a low TF-IDF score, meaning the word is less important to that particular document since the word also frequently appears in other documents. An example of a term like this would be \"the\", which is not very meaningful to any document.\n",
    "\n",
    "Whereas if a word appears in a document many times but it rarely appears in other documents, then it would mean that the word is important in that particular document.\n",
    "\n",
    "Another thing to note is that it is often important to pre-process text data such as removing stop words, lemmatize, etc. before vectorizing using TF-IDF in order to get better (or more useful) results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_args = {'strip_accents':'unicode',\n",
    "              'lowercase':True,\n",
    "              'stop_words':'english',\n",
    "              'ngram_range':(1,3)}\n",
    "tfidf_vectorizer = TfidfVectorizer(**tfidf_args)\n",
    "\n",
    "job_title_processed_tfidf = data['job_title'].apply(\n",
    "    process_text,\n",
    "    remove_stopwords=True,\n",
    "    lemmatize=True,\n",
    "    stem=True\n",
    ")\n",
    "\n",
    "keywords = [\"Aspiring human resources\", \"seeking human resources\"]\n",
    "keywords_processed_tfidf = [process_text(keyword) for keyword in keywords]\n",
    "\n",
    "data['fit_tfidf'] = cosine_similarity(tfidf_vectorizer.fit_transform(job_title_processed_tfidf),\n",
    "                                      tfidf_vectorizer.transform(keywords_processed_tfidf)).sum(axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For other vectorizers, still remove stopwords since stopwords do not add any values or meanings, but do not lemmatize or stem because such pre-processing could result in worse results when using models that have a pre-defined vocabulary or corpus - those models would not be able to provide meaningful word embedding when a lemma or stem is not found in the vocabulary, which can often happen since they are not a complete word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title_processed = data['job_title'].apply(\n",
    "    process_text,\n",
    "    remove_stopwords=True,\n",
    "    lemmatize=False,\n",
    "    stem=False\n",
    ")\n",
    "keywords_processed = [process_text(\n",
    "    keyword,\n",
    "    remove_stopwords=True,\n",
    "    lemmatize=False,\n",
    "    stem=False\n",
    "    ) for keyword in keywords]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-2. TensorFlow Tokenizer\n",
    "\n",
    "Tokenization is the process of breaking up a string into tokens. Commonly, these tokens are words, numbers, and/or punctuation.\n",
    "\n",
    "TensorFlow's Tokenizer class allows to vectorize a text corpus, by turning each text into either a sequence of integers (each integer being the index of a token in a dictionary) or into a vector where the coefficient for each token could be binary, based on word count, based on tf-idf, etc.\n",
    "\n",
    "By default, all punctuation is removed, turning the texts into space-separated sequences of words (words may include the ' character). These sequences are then split into lists of tokens. They will then be indexed or vectorized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(job_title_processed) # fit_on_texts updates internal vocabulary based on a list of texts; similar to tf-idf.\n",
    "data['fit_keras_tokenizer'] = cosine_similarity(tokenizer.texts_to_matrix(job_title_processed),\n",
    "                                                tokenizer.texts_to_matrix(keywords_processed)).sum(axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-3. GloVe (glove2word2vec)\n",
    "\n",
    "Gensim is an open-source library for unsupervised topic modeling, document indexing, retrieval by similarity, and other natural language processing functionalities, using modern statistical machine learning. It provides modules for training Word2Vec and other word embedding algorithms, and allows using pre-trained models.\n",
    "\n",
    "GloVe, coined from Global Vectors, is a model for distributed word representation. The model is an unsupervised learning algorithm for obtaining vector representations for words. This is achieved by mapping words into a meaningful space where the distance between words is related to semantic similarity.\n",
    "\n",
    "glove2word2vec allows to convert GloVe vectors into the word2vec. Both files are presented in text format and almost identical except that word2vec includes number of vectors and its dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove file source: https://nlp.stanford.edu/projects/glove/\n",
    "word2vec_file = get_tmpfile('word2vec.6B.50d.txt') # Create a temp file\n",
    "glove2word2vec('data/glove/glove.6B.50d.txt', word2vec_file) # Save glove2word2vec into the temp file\n",
    "glove_vectors = KeyedVectors.load_word2vec_format(word2vec_file) # Load the glove2word2vec from the teamp file\n",
    "glove_dimension = 50\n",
    "\n",
    "# Transform job titles and keywords into glove vectors\n",
    "glove_vectors_job_title = convert_words_to_vectors(job_title_processed, glove_vectors, glove_dimension)\n",
    "glove_vectors_keywords = convert_words_to_vectors(keywords_processed, glove_vectors, glove_dimension)\n",
    "data['fit_glove'] = cosine_similarity(glove_vectors_job_title, glove_vectors_keywords).sum(axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-4. Word2Vec\n",
    "\n",
    "some descriptions about word2vec to be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(sentences=job_title_processed.apply(lambda x: [word.lower() for word in x.split()]))\n",
    "word2vec_dimension = word2vec.vector_size\n",
    "\n",
    "# Transform job titles and keywords into word2vec vectors\n",
    "word2vec_job_title = convert_words_to_vectors(job_title_processed, word2vec, word2vec_dimension)\n",
    "word2vec_keywords = convert_words_to_vectors(keywords_processed, word2vec, word2vec_dimension)\n",
    "data['fit_word2vec'] = cosine_similarity(word2vec_job_title, word2vec_keywords).sum(axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-5. FastText\n",
    "\n",
    "some descriptions about fasttext to be added.\n",
    "\n",
    "One of the main disadvantages of Word2Vec and GloVe embedding is that they are unable to encode unknown or out-of-vocabulary words. So, to deal with this problem Facebook proposed a model FastText. It is an extension to Word2Vec and follows the same Skip-gram and CBOW model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext = FastText(sentences=job_title_processed.apply(lambda x: [word.lower() for word in x.split()]))\n",
    "fasttext_dimension = fasttext.vector_size\n",
    "\n",
    "# Transform job titles and keywords into fasttext vectors\n",
    "fasttext_job_title = convert_words_to_vectors(job_title_processed, fasttext, fasttext_dimension)\n",
    "fasttext_keywords = convert_words_to_vectors(keywords_processed, fasttext, fasttext_dimension)\n",
    "data['fit_fasttext'] = cosine_similarity(fasttext_job_title, fasttext_keywords).sum(axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. scale and evaluate fitness scores..\n",
    "\n",
    "Transform fit scores so that different fit scores will have the same range between 0 and 1.<br>\n",
    "This is for easier comparisons among different fit scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>fit</th>\n",
       "      <th>fit_tfidf</th>\n",
       "      <th>fit_keras_tokenizer</th>\n",
       "      <th>fit_glove</th>\n",
       "      <th>fit_word2vec</th>\n",
       "      <th>fit_fasttext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>104.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>104.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>52.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.328938</td>\n",
       "      <td>0.541808</td>\n",
       "      <td>0.604935</td>\n",
       "      <td>0.606050</td>\n",
       "      <td>0.586781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>30.166206</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.315271</td>\n",
       "      <td>0.359278</td>\n",
       "      <td>0.313345</td>\n",
       "      <td>0.323596</td>\n",
       "      <td>0.290476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>26.750000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.057644</td>\n",
       "      <td>0.106066</td>\n",
       "      <td>0.286359</td>\n",
       "      <td>0.425627</td>\n",
       "      <td>0.422400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>52.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.253802</td>\n",
       "      <td>0.642826</td>\n",
       "      <td>0.664575</td>\n",
       "      <td>0.720927</td>\n",
       "      <td>0.684321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>78.250000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.497634</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.864163</td>\n",
       "      <td>0.841687</td>\n",
       "      <td>0.776623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>104.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id  fit   fit_tfidf  fit_keras_tokenizer   fit_glove  \\\n",
       "count  104.000000  0.0  104.000000           104.000000  104.000000   \n",
       "mean    52.500000  NaN    0.328938             0.541808    0.604935   \n",
       "std     30.166206  NaN    0.315271             0.359278    0.313345   \n",
       "min      1.000000  NaN    0.000000             0.000000    0.000000   \n",
       "25%     26.750000  NaN    0.057644             0.106066    0.286359   \n",
       "50%     52.500000  NaN    0.253802             0.642826    0.664575   \n",
       "75%     78.250000  NaN    0.497634             0.800000    0.864163   \n",
       "max    104.000000  NaN    1.000000             1.000000    1.000000   \n",
       "\n",
       "       fit_word2vec  fit_fasttext  \n",
       "count    104.000000    104.000000  \n",
       "mean       0.606050      0.586781  \n",
       "std        0.323596      0.290476  \n",
       "min        0.000000      0.000000  \n",
       "25%        0.425627      0.422400  \n",
       "50%        0.720927      0.684321  \n",
       "75%        0.841687      0.776623  \n",
       "max        1.000000      1.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minmax_scaler = MinMaxScaler()\n",
    "fit_columns = [col for col in data.columns if \"fit_\" in col]\n",
    "data[fit_columns] = minmax_scaler.fit_transform(data[fit_columns])\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['fit'] = data[fit_columns].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: job_title, dtype: int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['fit']==0].job_title.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>connection</th>\n",
       "      <th>fit</th>\n",
       "      <th>fit_tfidf</th>\n",
       "      <th>fit_keras_tokenizer</th>\n",
       "      <th>fit_glove</th>\n",
       "      <th>fit_word2vec</th>\n",
       "      <th>fit_fasttext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Seeking Human Resources Opportunities</td>\n",
       "      <td>Chicago, Illinois</td>\n",
       "      <td>390</td>\n",
       "      <td>4.824715</td>\n",
       "      <td>0.921024</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.969821</td>\n",
       "      <td>0.933869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>Seeking Human Resources Opportunities</td>\n",
       "      <td>Chicago, Illinois</td>\n",
       "      <td>390</td>\n",
       "      <td>4.824715</td>\n",
       "      <td>0.921024</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.969821</td>\n",
       "      <td>0.933869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>Seeking Human Resources Position</td>\n",
       "      <td>Las Vegas, Nevada Area</td>\n",
       "      <td>48</td>\n",
       "      <td>4.759070</td>\n",
       "      <td>0.913385</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.953443</td>\n",
       "      <td>0.969821</td>\n",
       "      <td>0.922421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>Aspiring Human Resources Professional</td>\n",
       "      <td>Raleigh-Durham, North Carolina Area</td>\n",
       "      <td>44</td>\n",
       "      <td>4.737427</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.920193</td>\n",
       "      <td>0.859645</td>\n",
       "      <td>0.957589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Aspiring Human Resources Professional</td>\n",
       "      <td>Raleigh-Durham, North Carolina Area</td>\n",
       "      <td>44</td>\n",
       "      <td>4.737427</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.920193</td>\n",
       "      <td>0.859645</td>\n",
       "      <td>0.957589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                              job_title  \\\n",
       "27  28  Seeking Human Resources Opportunities   \n",
       "29  30  Seeking Human Resources Opportunities   \n",
       "98  99       Seeking Human Resources Position   \n",
       "32  33  Aspiring Human Resources Professional   \n",
       "2    3  Aspiring Human Resources Professional   \n",
       "\n",
       "                               location connection       fit  fit_tfidf  \\\n",
       "27                    Chicago, Illinois        390  4.824715   0.921024   \n",
       "29                    Chicago, Illinois        390  4.824715   0.921024   \n",
       "98               Las Vegas, Nevada Area         48  4.759070   0.913385   \n",
       "32  Raleigh-Durham, North Carolina Area         44  4.737427   1.000000   \n",
       "2   Raleigh-Durham, North Carolina Area         44  4.737427   1.000000   \n",
       "\n",
       "    fit_keras_tokenizer  fit_glove  fit_word2vec  fit_fasttext  \n",
       "27                  1.0   1.000000      0.969821      0.933869  \n",
       "29                  1.0   1.000000      0.969821      0.933869  \n",
       "98                  1.0   0.953443      0.969821      0.922421  \n",
       "32                  1.0   0.920193      0.859645      0.957589  \n",
       "2                   1.0   0.920193      0.859645      0.957589  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sort_values('fit', ascending=False).head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the job titles for the candidates who got the highest fitness score, they indeed look very relevant - in fact the job titles include one of the exact keywords \"Aspiring Human Resources\" in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>connection</th>\n",
       "      <th>fit</th>\n",
       "      <th>fit_tfidf</th>\n",
       "      <th>fit_keras_tokenizer</th>\n",
       "      <th>fit_glove</th>\n",
       "      <th>fit_word2vec</th>\n",
       "      <th>fit_fasttext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>Advisory Board Member at Celal Bayar University</td>\n",
       "      <td>İzmir, Türkiye</td>\n",
       "      <td>500+</td>\n",
       "      <td>0.252493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.082857</td>\n",
       "      <td>0.169637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>48</td>\n",
       "      <td>Advisory Board Member at Celal Bayar University</td>\n",
       "      <td>İzmir, Türkiye</td>\n",
       "      <td>500+</td>\n",
       "      <td>0.252493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.082857</td>\n",
       "      <td>0.169637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Advisory Board Member at Celal Bayar University</td>\n",
       "      <td>İzmir, Türkiye</td>\n",
       "      <td>500+</td>\n",
       "      <td>0.252493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.082857</td>\n",
       "      <td>0.169637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>Advisory Board Member at Celal Bayar University</td>\n",
       "      <td>İzmir, Türkiye</td>\n",
       "      <td>500+</td>\n",
       "      <td>0.252493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.082857</td>\n",
       "      <td>0.169637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>87</td>\n",
       "      <td>Bachelor of Science in Biology from Victoria U...</td>\n",
       "      <td>Baltimore, Maryland</td>\n",
       "      <td>40</td>\n",
       "      <td>0.257883</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05825</td>\n",
       "      <td>0.082857</td>\n",
       "      <td>0.116777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                          job_title  \\\n",
       "34  35    Advisory Board Member at Celal Bayar University   \n",
       "47  48    Advisory Board Member at Celal Bayar University   \n",
       "4    5    Advisory Board Member at Celal Bayar University   \n",
       "22  23    Advisory Board Member at Celal Bayar University   \n",
       "86  87  Bachelor of Science in Biology from Victoria U...   \n",
       "\n",
       "               location connection       fit  fit_tfidf  fit_keras_tokenizer  \\\n",
       "34       İzmir, Türkiye      500+   0.252493        0.0                  0.0   \n",
       "47       İzmir, Türkiye      500+   0.252493        0.0                  0.0   \n",
       "4        İzmir, Türkiye      500+   0.252493        0.0                  0.0   \n",
       "22       İzmir, Türkiye      500+   0.252493        0.0                  0.0   \n",
       "86  Baltimore, Maryland         40  0.257883        0.0                  0.0   \n",
       "\n",
       "    fit_glove  fit_word2vec  fit_fasttext  \n",
       "34    0.00000      0.082857      0.169637  \n",
       "47    0.00000      0.082857      0.169637  \n",
       "4     0.00000      0.082857      0.169637  \n",
       "22    0.00000      0.082857      0.169637  \n",
       "86    0.05825      0.082857      0.116777  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sort_values('fit', ascending=True).head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. drop irrelevant candidates and present the top candidates\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['fit']==0].job_title.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>fit</th>\n",
       "      <th>fit_tfidf</th>\n",
       "      <th>fit_keras_tokenizer</th>\n",
       "      <th>fit_glove</th>\n",
       "      <th>fit_word2vec</th>\n",
       "      <th>fit_fasttext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>104.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>104.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>52.500000</td>\n",
       "      <td>2.668511</td>\n",
       "      <td>0.328938</td>\n",
       "      <td>0.541808</td>\n",
       "      <td>0.604935</td>\n",
       "      <td>0.606050</td>\n",
       "      <td>0.586781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>30.166206</td>\n",
       "      <td>1.491816</td>\n",
       "      <td>0.315271</td>\n",
       "      <td>0.359278</td>\n",
       "      <td>0.313345</td>\n",
       "      <td>0.323596</td>\n",
       "      <td>0.290476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.252493</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>26.750000</td>\n",
       "      <td>1.565605</td>\n",
       "      <td>0.057644</td>\n",
       "      <td>0.106066</td>\n",
       "      <td>0.286359</td>\n",
       "      <td>0.425627</td>\n",
       "      <td>0.422400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>52.500000</td>\n",
       "      <td>2.923056</td>\n",
       "      <td>0.253802</td>\n",
       "      <td>0.642826</td>\n",
       "      <td>0.664575</td>\n",
       "      <td>0.720927</td>\n",
       "      <td>0.684321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>78.250000</td>\n",
       "      <td>3.534160</td>\n",
       "      <td>0.497634</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.864163</td>\n",
       "      <td>0.841687</td>\n",
       "      <td>0.776623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>104.000000</td>\n",
       "      <td>4.824715</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id         fit   fit_tfidf  fit_keras_tokenizer   fit_glove  \\\n",
       "count  104.000000  104.000000  104.000000           104.000000  104.000000   \n",
       "mean    52.500000    2.668511    0.328938             0.541808    0.604935   \n",
       "std     30.166206    1.491816    0.315271             0.359278    0.313345   \n",
       "min      1.000000    0.252493    0.000000             0.000000    0.000000   \n",
       "25%     26.750000    1.565605    0.057644             0.106066    0.286359   \n",
       "50%     52.500000    2.923056    0.253802             0.642826    0.664575   \n",
       "75%     78.250000    3.534160    0.497634             0.800000    0.864163   \n",
       "max    104.000000    4.824715    1.000000             1.000000    1.000000   \n",
       "\n",
       "       fit_word2vec  fit_fasttext  \n",
       "count    104.000000    104.000000  \n",
       "mean       0.606050      0.586781  \n",
       "std        0.323596      0.290476  \n",
       "min        0.000000      0.000000  \n",
       "25%        0.425627      0.422400  \n",
       "50%        0.720927      0.684321  \n",
       "75%        0.841687      0.776623  \n",
       "max        1.000000      1.000000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No candidates with a fitness score of 0 although the min fit score of each of the fit_columns is all 0.\n",
    "\n",
    "Add a filter column 'has_zero_scores' for candidates with at least 1 'zero' fitness score from the fit_columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_zero_scores = []\n",
    "for i, row in data.iterrows():\n",
    "    has_zero_score = 0\n",
    "    for fit in data.iloc[i][fit_columns]:\n",
    "        if fit == 0:\n",
    "            has_zero_score = 1\n",
    "    \n",
    "    has_zero_scores.append(has_zero_score)\n",
    "\n",
    "data['has_zero_scores'] = has_zero_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Native English Teacher at EPIK (English Program in Korea)',\n",
       "       'Advisory Board Member at Celal Bayar University',\n",
       "       'Student at Chapman University',\n",
       "       'Junior MES Engineer| Information Systems',\n",
       "       'RRP Brand Portfolio Executive at JTI (Japan Tobacco International)',\n",
       "       'Information Systems Specialist and Programmer with a love for data and organization.',\n",
       "       'Bachelor of Science in Biology from Victoria University of Wellington',\n",
       "       'Undergraduate Research Assistant at Styczynski Lab',\n",
       "       'Lead Official at Western Illinois University',\n",
       "       'Admissions Representative at Community medical center long beach',\n",
       "       'Student at Westfield State University',\n",
       "       'Student at Indiana University Kokomo - Business Management - Retail Manager at Delphi Hardware and Paint',\n",
       "       'Student', 'Business Intelligence and Analytics at Travelers',\n",
       "       'Always set them up for Success',\n",
       "       'Director Of Administration at Excellence Logging'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['has_zero_scores'] == 1].job_title.unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can drop these values as they indeed seem irrelavant to our keywords, \"Aspiring human resources\" and \"seeking human resources\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_tfidf least fit job title 1: Director Of Administration at Excellence Logging\n",
      "fit_tfidf least fit job title 2: Native English Teacher at EPIK (English Program in Korea)\n",
      "fit_tfidf least fit job title 3: Bachelor of Science in Biology from Victoria University of Wellington\n",
      "fit_tfidf least fit job title 4: Student at Chapman University\n",
      "fit_tfidf least fit job title 5: Advisory Board Member at Celal Bayar University\n",
      "\n",
      "fit_keras_tokenizer least fit job title 1: Director Of Administration at Excellence Logging\n",
      "fit_keras_tokenizer least fit job title 2: Native English Teacher at EPIK (English Program in Korea)\n",
      "fit_keras_tokenizer least fit job title 3: Advisory Board Member at Celal Bayar University\n",
      "fit_keras_tokenizer least fit job title 4: Native English Teacher at EPIK (English Program in Korea)\n",
      "fit_keras_tokenizer least fit job title 5: Advisory Board Member at Celal Bayar University\n",
      "\n",
      "fit_glove least fit job title 1: Advisory Board Member at Celal Bayar University\n",
      "fit_glove least fit job title 2: Advisory Board Member at Celal Bayar University\n",
      "fit_glove least fit job title 3: Advisory Board Member at Celal Bayar University\n",
      "fit_glove least fit job title 4: Advisory Board Member at Celal Bayar University\n",
      "fit_glove least fit job title 5: Student at Chapman University\n",
      "\n",
      "fit_word2vec least fit job title 1: Junior MES Engineer| Information Systems\n",
      "fit_word2vec least fit job title 2: Native English Teacher at EPIK (English Program in Korea)\n",
      "fit_word2vec least fit job title 3: Native English Teacher at EPIK (English Program in Korea)\n",
      "fit_word2vec least fit job title 4: Native English Teacher at EPIK (English Program in Korea)\n",
      "fit_word2vec least fit job title 5: Native English Teacher at EPIK (English Program in Korea)\n",
      "\n",
      "fit_fasttext least fit job title 1: Student at Westfield State University\n",
      "fit_fasttext least fit job title 2: Always set them up for Success\n",
      "fit_fasttext least fit job title 3: Director Of Administration at Excellence Logging\n",
      "fit_fasttext least fit job title 4: Junior MES Engineer| Information Systems\n",
      "fit_fasttext least fit job title 5: Business Intelligence and Analytics at Travelers\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fit_col in fit_columns:\n",
    "    for i, job_title in enumerate(data.sort_values(fit_col).head().job_title.values):\n",
    "        print(f\"{fit_col} least fit job title {i+1}: {job_title}\")\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the job titles with the worst fitness scores, each evaluation methods for fitness seems to perform fine - i.e. all those 'worst' job titles do not seem relevant to our keywords. In other words, it would be safe to discard candidates with zero fitness scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered = data[ data['has_zero_scores'] != 1 ].drop('has_zero_scores', axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are top 20 'best-fit' candidates and their job titles, after dropping candidates with at least 1 zero fitness scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique job titles of top 20 candidates:\n",
      "['Seeking Human Resources Opportunities'\n",
      " 'Seeking Human Resources Position'\n",
      " 'Aspiring Human Resources Professional'\n",
      " 'Aspiring Human Resources Manager, seeking internship in Human Resources.'\n",
      " 'Aspiring Human Resources Specialist'\n",
      " 'Seeking Human Resources Human Resources Information System and Generalist Positions']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>connection</th>\n",
       "      <th>fit</th>\n",
       "      <th>fit_tfidf</th>\n",
       "      <th>fit_keras_tokenizer</th>\n",
       "      <th>fit_glove</th>\n",
       "      <th>fit_word2vec</th>\n",
       "      <th>fit_fasttext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>Seeking Human Resources Opportunities</td>\n",
       "      <td>Chicago, Illinois</td>\n",
       "      <td>390</td>\n",
       "      <td>4.824715</td>\n",
       "      <td>0.921024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.969821</td>\n",
       "      <td>0.933869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>Seeking Human Resources Opportunities</td>\n",
       "      <td>Chicago, Illinois</td>\n",
       "      <td>390</td>\n",
       "      <td>4.824715</td>\n",
       "      <td>0.921024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.969821</td>\n",
       "      <td>0.933869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99</td>\n",
       "      <td>Seeking Human Resources Position</td>\n",
       "      <td>Las Vegas, Nevada Area</td>\n",
       "      <td>48</td>\n",
       "      <td>4.759070</td>\n",
       "      <td>0.913385</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.953443</td>\n",
       "      <td>0.969821</td>\n",
       "      <td>0.922421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Aspiring Human Resources Professional</td>\n",
       "      <td>Raleigh-Durham, North Carolina Area</td>\n",
       "      <td>44</td>\n",
       "      <td>4.737427</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.920193</td>\n",
       "      <td>0.859645</td>\n",
       "      <td>0.957589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>Aspiring Human Resources Professional</td>\n",
       "      <td>Raleigh-Durham, North Carolina Area</td>\n",
       "      <td>44</td>\n",
       "      <td>4.737427</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.920193</td>\n",
       "      <td>0.859645</td>\n",
       "      <td>0.957589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21</td>\n",
       "      <td>Aspiring Human Resources Professional</td>\n",
       "      <td>Raleigh-Durham, North Carolina Area</td>\n",
       "      <td>44</td>\n",
       "      <td>4.737427</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.920193</td>\n",
       "      <td>0.859645</td>\n",
       "      <td>0.957589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>33</td>\n",
       "      <td>Aspiring Human Resources Professional</td>\n",
       "      <td>Raleigh-Durham, North Carolina Area</td>\n",
       "      <td>44</td>\n",
       "      <td>4.737427</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.920193</td>\n",
       "      <td>0.859645</td>\n",
       "      <td>0.957589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>46</td>\n",
       "      <td>Aspiring Human Resources Professional</td>\n",
       "      <td>Raleigh-Durham, North Carolina Area</td>\n",
       "      <td>44</td>\n",
       "      <td>4.737427</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.920193</td>\n",
       "      <td>0.859645</td>\n",
       "      <td>0.957589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>58</td>\n",
       "      <td>Aspiring Human Resources Professional</td>\n",
       "      <td>Raleigh-Durham, North Carolina Area</td>\n",
       "      <td>44</td>\n",
       "      <td>4.737427</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.920193</td>\n",
       "      <td>0.859645</td>\n",
       "      <td>0.957589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>97</td>\n",
       "      <td>Aspiring Human Resources Professional</td>\n",
       "      <td>Kokomo, Indiana Area</td>\n",
       "      <td>71</td>\n",
       "      <td>4.737427</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.920193</td>\n",
       "      <td>0.859645</td>\n",
       "      <td>0.957589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>73</td>\n",
       "      <td>Aspiring Human Resources Manager, seeking inte...</td>\n",
       "      <td>Houston, Texas Area</td>\n",
       "      <td>7</td>\n",
       "      <td>4.609975</td>\n",
       "      <td>0.648168</td>\n",
       "      <td>0.979796</td>\n",
       "      <td>0.982011</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6</td>\n",
       "      <td>Aspiring Human Resources Specialist</td>\n",
       "      <td>Greater New York City Area</td>\n",
       "      <td>1</td>\n",
       "      <td>4.542606</td>\n",
       "      <td>0.830646</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962413</td>\n",
       "      <td>0.836157</td>\n",
       "      <td>0.913391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>24</td>\n",
       "      <td>Aspiring Human Resources Specialist</td>\n",
       "      <td>Greater New York City Area</td>\n",
       "      <td>1</td>\n",
       "      <td>4.542606</td>\n",
       "      <td>0.830646</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962413</td>\n",
       "      <td>0.836157</td>\n",
       "      <td>0.913391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>36</td>\n",
       "      <td>Aspiring Human Resources Specialist</td>\n",
       "      <td>Greater New York City Area</td>\n",
       "      <td>1</td>\n",
       "      <td>4.542606</td>\n",
       "      <td>0.830646</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962413</td>\n",
       "      <td>0.836157</td>\n",
       "      <td>0.913391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>49</td>\n",
       "      <td>Aspiring Human Resources Specialist</td>\n",
       "      <td>Greater New York City Area</td>\n",
       "      <td>1</td>\n",
       "      <td>4.542606</td>\n",
       "      <td>0.830646</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962413</td>\n",
       "      <td>0.836157</td>\n",
       "      <td>0.913391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>60</td>\n",
       "      <td>Aspiring Human Resources Specialist</td>\n",
       "      <td>Greater New York City Area</td>\n",
       "      <td>1</td>\n",
       "      <td>4.542606</td>\n",
       "      <td>0.830646</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962413</td>\n",
       "      <td>0.836157</td>\n",
       "      <td>0.913391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10</td>\n",
       "      <td>Seeking Human Resources Human Resources Inform...</td>\n",
       "      <td>Greater Philadelphia Area</td>\n",
       "      <td>500+</td>\n",
       "      <td>4.285356</td>\n",
       "      <td>0.736714</td>\n",
       "      <td>0.755929</td>\n",
       "      <td>0.953667</td>\n",
       "      <td>0.928727</td>\n",
       "      <td>0.910319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>40</td>\n",
       "      <td>Seeking Human Resources Human Resources Inform...</td>\n",
       "      <td>Greater Philadelphia Area</td>\n",
       "      <td>500+</td>\n",
       "      <td>4.285356</td>\n",
       "      <td>0.736714</td>\n",
       "      <td>0.755929</td>\n",
       "      <td>0.953667</td>\n",
       "      <td>0.928727</td>\n",
       "      <td>0.910319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>53</td>\n",
       "      <td>Seeking Human Resources Human Resources Inform...</td>\n",
       "      <td>Greater Philadelphia Area</td>\n",
       "      <td>500+</td>\n",
       "      <td>4.285356</td>\n",
       "      <td>0.736714</td>\n",
       "      <td>0.755929</td>\n",
       "      <td>0.953667</td>\n",
       "      <td>0.928727</td>\n",
       "      <td>0.910319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>62</td>\n",
       "      <td>Seeking Human Resources Human Resources Inform...</td>\n",
       "      <td>Greater Philadelphia Area</td>\n",
       "      <td>500+</td>\n",
       "      <td>4.285356</td>\n",
       "      <td>0.736714</td>\n",
       "      <td>0.755929</td>\n",
       "      <td>0.953667</td>\n",
       "      <td>0.928727</td>\n",
       "      <td>0.910319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                          job_title  \\\n",
       "0   28              Seeking Human Resources Opportunities   \n",
       "1   30              Seeking Human Resources Opportunities   \n",
       "2   99                   Seeking Human Resources Position   \n",
       "3    3              Aspiring Human Resources Professional   \n",
       "4   17              Aspiring Human Resources Professional   \n",
       "5   21              Aspiring Human Resources Professional   \n",
       "6   33              Aspiring Human Resources Professional   \n",
       "7   46              Aspiring Human Resources Professional   \n",
       "8   58              Aspiring Human Resources Professional   \n",
       "9   97              Aspiring Human Resources Professional   \n",
       "10  73  Aspiring Human Resources Manager, seeking inte...   \n",
       "11   6                Aspiring Human Resources Specialist   \n",
       "12  24                Aspiring Human Resources Specialist   \n",
       "13  36                Aspiring Human Resources Specialist   \n",
       "14  49                Aspiring Human Resources Specialist   \n",
       "15  60                Aspiring Human Resources Specialist   \n",
       "16  10  Seeking Human Resources Human Resources Inform...   \n",
       "17  40  Seeking Human Resources Human Resources Inform...   \n",
       "18  53  Seeking Human Resources Human Resources Inform...   \n",
       "19  62  Seeking Human Resources Human Resources Inform...   \n",
       "\n",
       "                               location connection       fit  fit_tfidf  \\\n",
       "0                     Chicago, Illinois        390  4.824715   0.921024   \n",
       "1                     Chicago, Illinois        390  4.824715   0.921024   \n",
       "2                Las Vegas, Nevada Area         48  4.759070   0.913385   \n",
       "3   Raleigh-Durham, North Carolina Area         44  4.737427   1.000000   \n",
       "4   Raleigh-Durham, North Carolina Area         44  4.737427   1.000000   \n",
       "5   Raleigh-Durham, North Carolina Area         44  4.737427   1.000000   \n",
       "6   Raleigh-Durham, North Carolina Area         44  4.737427   1.000000   \n",
       "7   Raleigh-Durham, North Carolina Area         44  4.737427   1.000000   \n",
       "8   Raleigh-Durham, North Carolina Area         44  4.737427   1.000000   \n",
       "9                  Kokomo, Indiana Area         71  4.737427   1.000000   \n",
       "10                  Houston, Texas Area          7  4.609975   0.648168   \n",
       "11           Greater New York City Area          1  4.542606   0.830646   \n",
       "12           Greater New York City Area          1  4.542606   0.830646   \n",
       "13           Greater New York City Area          1  4.542606   0.830646   \n",
       "14           Greater New York City Area          1  4.542606   0.830646   \n",
       "15           Greater New York City Area          1  4.542606   0.830646   \n",
       "16            Greater Philadelphia Area      500+   4.285356   0.736714   \n",
       "17            Greater Philadelphia Area      500+   4.285356   0.736714   \n",
       "18            Greater Philadelphia Area      500+   4.285356   0.736714   \n",
       "19            Greater Philadelphia Area      500+   4.285356   0.736714   \n",
       "\n",
       "    fit_keras_tokenizer  fit_glove  fit_word2vec  fit_fasttext  \n",
       "0              1.000000   1.000000      0.969821      0.933869  \n",
       "1              1.000000   1.000000      0.969821      0.933869  \n",
       "2              1.000000   0.953443      0.969821      0.922421  \n",
       "3              1.000000   0.920193      0.859645      0.957589  \n",
       "4              1.000000   0.920193      0.859645      0.957589  \n",
       "5              1.000000   0.920193      0.859645      0.957589  \n",
       "6              1.000000   0.920193      0.859645      0.957589  \n",
       "7              1.000000   0.920193      0.859645      0.957589  \n",
       "8              1.000000   0.920193      0.859645      0.957589  \n",
       "9              1.000000   0.920193      0.859645      0.957589  \n",
       "10             0.979796   0.982011      1.000000      1.000000  \n",
       "11             1.000000   0.962413      0.836157      0.913391  \n",
       "12             1.000000   0.962413      0.836157      0.913391  \n",
       "13             1.000000   0.962413      0.836157      0.913391  \n",
       "14             1.000000   0.962413      0.836157      0.913391  \n",
       "15             1.000000   0.962413      0.836157      0.913391  \n",
       "16             0.755929   0.953667      0.928727      0.910319  \n",
       "17             0.755929   0.953667      0.928727      0.910319  \n",
       "18             0.755929   0.953667      0.928727      0.910319  \n",
       "19             0.755929   0.953667      0.928727      0.910319  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filtered = data_filtered.sort_values(['fit', 'id', 'connection'], ascending=[False, True, True]).reset_index(drop=True)\n",
    "print(f\"Unique job titles of top 20 candidates:\\n{data_filtered.head(20).job_title.unique()}\")\n",
    "data_filtered.head(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Train ranking models - XGBoost and LGBM Rankers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize job titles using fasttext and use the word vectors as training features.<br>\n",
    "Set the 'id' column as index, and the fitness score of each candidate under the 'fit' column will be the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features = pd.DataFrame(\n",
    "    get_word_vectors(data_filtered, 'job_title', vectorizer='fasttext',\n",
    "                     to_process_text=True, remove_stopwords=True, lemmatize=False, stem=False)\n",
    ")\n",
    "data_selected = pd.concat([data_filtered, training_features], axis=1).set_index('id')\n",
    "X = data_selected[training_features.columns]\n",
    "y = data_selected['fit']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train and test sets before train any model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "random_state = 1\n",
    "X_train, X_test, y_train_fitness, y_test_fitness = split_data(\n",
    "    X, y, test_size, random_state=random_state, oversampling=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert fitness scores into ranks for y_train and y_test separately so that each data set has ranks starting from 1 to the number of data points. Also change the name of the series (or column) from 'fit' to 'rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train_fitness.rank(method='dense', ascending=False)\n",
    "y_train.name = 'rank'\n",
    "\n",
    "y_test = y_test_fitness.rank(method='dense', ascending=False)\n",
    "y_test.name = 'rank'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6-1. Train an XGBoost Ranker, and get prediction results.\n",
    "\n",
    "descriptions about XGBoost to be added.\n",
    "* Evaluation metric: NDCG (normalized discounted cumulative gain) is a measure of the effectiveness of a ranking system, taking into account the position of relevant items in the ranked list. It is based on the idea that items that are higher in the ranking should be given more credit than items that are lower in the ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth stats:\n",
      "                      y_train  y_test\n",
      "Mean (Top 5 rankers)   2.7368  3.3750\n",
      "Mean                  15.0323  6.1250\n",
      "Std                   10.4880  3.2838 \n",
      "\n",
      "Train stats:\n",
      "Mean rank of top 5 candidates based on predictions: 5.1579\n",
      "Mean rank of all candidates based on predictions: 15.0806\n",
      "Std rank of all candidates based on predictions: 9.8568\n",
      "Mean absolute difference between each pair of rank and predicted rank: 2.9194\n",
      "    rank  pred_rank  abs_diff\n",
      "id                           \n",
      "28   1.0        1.0       0.0\n",
      "30   1.0        1.0       0.0\n",
      "27   6.0        2.0       4.0\n",
      "29   6.0        2.0       4.0\n",
      "78  10.0        3.0       7.0 \n",
      "\n",
      "Test stats:\n",
      "Mean rank of top 5 candidates based on predictions: 3.125\n",
      "Mean rank of all candidates based on predictions: 6.125\n",
      "Std rank of all candidates based on predictions: 3.6309\n",
      "Mean absolute difference between each pair of rank and predicted rank: 1.625\n",
      "    rank  pred_rank  abs_diff\n",
      "id                           \n",
      "99   1.0        1.0       0.0\n",
      "73   2.0        2.0       0.0\n",
      "9    4.0        3.0       1.0\n",
      "50   4.0        3.0       1.0\n",
      "7    4.0        3.0       1.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_params = {\n",
    "    # 'n_estimators': 40,\n",
    "    # 'max_depth': 2,\n",
    "    # 'learning_rate': 0.02, # same as xgb's eta; default=0.3\n",
    "    'objective': 'rank:pairwise', # perform better than 'rank:ndcg'\n",
    "    'booster': 'gbtree',\n",
    "    'eval_metric': 'ndcg',\n",
    "    # 'subsample': 0.5,\n",
    "    # 'gamma': 4.5, # default=0; the larger the more conservative\n",
    "    # 'min_child_weight': 2, # default=1; the larger the more conservative\n",
    "    'random_state': random_state\n",
    "}\n",
    "\n",
    "xgb_ranker = XGBRanker(**xgb_params)\n",
    "xgb_ranker.fit(X_train, y_train,\n",
    "               group=y_train.value_counts(),\n",
    "               eval_set=[(X_test, y_test)],\n",
    "               eval_group=[y_test.value_counts()]\n",
    "               )\n",
    "\n",
    "stats_df = pd.DataFrame(\n",
    "    index=[\"Mean (Top 5 rankers)\", \"Mean\", \"Std\"],\n",
    "    columns=[\"y_train\", \"y_test\"],\n",
    "    data=[\n",
    "        [round(y_train[y_train<=5].mean(), 4),\n",
    "         round(y_test[y_test<=5].mean(), 4)],\n",
    "        [round(y_train.mean(), 4),\n",
    "         round(y_test.mean(), 4)],\n",
    "        [round(y_train.std(), 4),\n",
    "         round(y_test.std(), 4)]\n",
    "    ]\n",
    ")\n",
    "print(\"Ground truth stats:\")\n",
    "print(stats_df,\"\\n\")\n",
    "\n",
    "print(\"Train stats:\")\n",
    "xgb_train_result = get_rank_predictions(X_train, y_train, xgb_ranker, target_column=\"rank\", target=\"candidates\")\n",
    "\n",
    "print(\"Test stats:\")\n",
    "xgb_test_result = get_rank_predictions(X_test, y_test, xgb_ranker, target_column=\"rank\", target=\"candidates\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6-2. Train an LGBM (Light Gradient-Boosting Machine) Ranker, and get prediction results.\n",
    "\n",
    "descriptions about LGBM to be added.\n",
    "* Ranking algorithm: LambdaRank. This is a technique where ranking is transformed into a pairwise classification or regression problem. Basically, the algorithms consider a pair of items at a single time to come up with a viable ordering of those items before initiating the final order of the entire list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth stats:\n",
      "                      y_train  y_test\n",
      "Mean (Top 5 rankers)   2.7368  3.3750\n",
      "Mean                  15.0323  6.1250\n",
      "Std                   10.4880  3.2838 \n",
      "\n",
      "Train stats:\n",
      "Mean rank of top 5 candidates based on predictions: 5.0526\n",
      "Mean rank of all candidates based on predictions: 15.1129\n",
      "Std rank of all candidates based on predictions: 10.1934\n",
      "Mean absolute difference between each pair of rank and predicted rank: 2.3387\n",
      "    rank  pred_rank  abs_diff\n",
      "id                           \n",
      "27   6.0        1.0       5.0\n",
      "29   6.0        1.0       5.0\n",
      "28   1.0        2.0       1.0\n",
      "30   1.0        2.0       1.0\n",
      "94   7.0        3.0       4.0 \n",
      "\n",
      "Test stats:\n",
      "Mean rank of top 5 candidates based on predictions: 3.5\n",
      "Mean rank of all candidates based on predictions: 6.25\n",
      "Std rank of all candidates based on predictions: 3.3961\n",
      "Mean absolute difference between each pair of rank and predicted rank: 1.375\n",
      "    rank  pred_rank  abs_diff\n",
      "id                           \n",
      "73   2.0        1.0       1.0\n",
      "66   3.0        2.0       1.0\n",
      "99   1.0        3.0       2.0\n",
      "9    4.0        4.0       0.0\n",
      "50   4.0        4.0       0.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lgbm_ranker = LGBMRanker(\n",
    "    boosting_type=\"dart\", # 'gbdt', 'dart', 'rf'\n",
    "    # max_depth=2,\n",
    "    objective=\"lambdarank\",\n",
    "    metric= \"ndcg\",\n",
    "    label_gain =[i for i in range(int(max(y_train.max(), y_test.max())) + 2)],\n",
    "    random_state=random_state\n",
    "    )\n",
    "\n",
    "lgbm_ranker.fit(\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    group=y_train.value_counts(),\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    eval_group=[y_test.value_counts()],\n",
    "    verbose=-1\n",
    "    )\n",
    "\n",
    "print(\"Ground truth stats:\")\n",
    "print(stats_df,\"\\n\")\n",
    "\n",
    "print(\"Train stats:\")\n",
    "lgbm_train_result = get_rank_predictions(X_train, y_train, lgbm_ranker, target_column=\"rank\", target=\"candidates\")\n",
    "\n",
    "print(\"Test stats:\")\n",
    "lgbm_test_result = get_rank_predictions(X_test, y_test, lgbm_ranker, target_column=\"rank\", target=\"candidates\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both rankers successfully predicted the top-ranked candidates (i.e. pred_rank == 1.0) as they all were selected candidates.\n",
    "\n",
    "In terms of the mean of predicted ranks for selected candidates, the LGBM ranker performed better at predicting selected candidates. Therefore, the model will be used instead of the XGB ranker for the rest of the project.\n",
    "\n",
    "* Overall, the predictions by the LGBM ranker were quite close to the ground truths without much hyperparameter tuning, even though some hyperparameter tuning was done for the XGB ranker."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Star ideal candidates and re-train the ranking model based on the updated ranks/criteria.\n",
    "\n",
    "Since we have built our base ranking model, proceed to starring ideal candidates and re-rank all candidates based on the stars. Starring one candidate sets this candidate as an ideal candidate for the given role. The list of candidates will be re-ranked each time a candidate or a list of candidates is starred."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7-1. Get the ids of ideal candidates (i.e. the candidates to star).\n",
    "\n",
    "Since we cannot actually take input from HR, randomly choose 5 candidates as ideal candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(random_state)\n",
    "input_ids = random.choices(y.index, k=5)\n",
    "ideal_candidates = sorted([id for id in input_ids])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7-2. Create a copy of the train and test data to re-rank candidates based on starring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_updated = X_train.copy()\n",
    "X_test_updated = X_test.copy()\n",
    "y_train_updated = y_train.copy()\n",
    "y_test_updated = y_test.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7-3. Add a binary feature 'star' to X_train and X_test.\n",
    "\n",
    "If a candidate is 'starred', the feature value will be 1, otherwise 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_updated['star'] = [1 if idx in ideal_candidates else 0 for idx in X_train_updated.index]\n",
    "X_test_updated['star'] = [1 if idx in ideal_candidates else 0 for idx in X_test_updated.index]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7-4. Add 1 to the ranks of all candidates (e.g. rank 1 will become rank 2) so that the starred candidates can become the top ranker with a rank of 1.\n",
    "\n",
    "Update y_train and y_test with the updated ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank of candidate 13 in y_test updated to 1.\n",
      "Rank of candidate 15 in y_train updated to 1.\n",
      "Rank of candidate 50 in y_test updated to 1.\n",
      "Rank of candidate 62 in y_train updated to 1.\n",
      "Rank of candidate 73 in y_test updated to 1.\n"
     ]
    }
   ],
   "source": [
    "y_train_updated += 1\n",
    "y_test_updated += 1\n",
    "\n",
    "ideal_rank = 1\n",
    "for id in ideal_candidates:\n",
    "    if id in y_train_updated.index:\n",
    "        y_train_updated[id] = ideal_rank\n",
    "        print(f\"Rank of candidate {id} in y_train updated to {ideal_rank}.\")\n",
    "    elif id in y_test.index:\n",
    "        y_test_updated[id] = ideal_rank\n",
    "        print(f\"Rank of candidate {id} in y_test updated to {ideal_rank}.\")\n",
    "    else:\n",
    "        print(f\"Candidate {id} not found!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7-5. Re-train ranking models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Updated) Ground truth stats:\n",
      "                      y_train_updated  y_test_updated\n",
      "Mean (Top 5 rankers)           3.2632          3.0000\n",
      "Mean                          15.5000          6.2500\n",
      "Std                           10.5795          3.9749 \n",
      "\n",
      "(Updated) Train stats:\n",
      "Mean rank of top 5 candidates based on predictions: 3.8947\n",
      "Mean rank of all candidates based on predictions: 15.8387\n",
      "Std rank of all candidates based on predictions: 10.5663\n",
      "Mean absolute difference between each pair of rank and predicted rank: 2.9194\n",
      "    rank  pred_rank  abs_diff\n",
      "id                           \n",
      "3    3.0        1.0       2.0\n",
      "58   3.0        1.0       2.0\n",
      "46   3.0        1.0       2.0\n",
      "17   3.0        1.0       2.0\n",
      "33   3.0        1.0       2.0 \n",
      "\n",
      "(Updated) Test stats:\n",
      "Mean rank of top 5 candidates based on predictions: 4.625\n",
      "Mean rank of all candidates based on predictions: 7.1875\n",
      "Std rank of all candidates based on predictions: 3.987\n",
      "Mean absolute difference between each pair of rank and predicted rank: 1.9375\n",
      "    rank  pred_rank  abs_diff\n",
      "id                           \n",
      "73   1.0        1.0       0.0\n",
      "99   2.0        2.0       0.0\n",
      "50   1.0        3.0       2.0\n",
      "66   4.0        4.0       0.0\n",
      "9    5.0        5.0       0.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_ranker.fit(X_train_updated, y_train_updated,\n",
    "               group=y_train_updated.value_counts(),\n",
    "               eval_set=[(X_test_updated, y_test_updated)],\n",
    "               eval_group=[y_test_updated.value_counts()]\n",
    "               )\n",
    "\n",
    "stats_df_updated = pd.DataFrame(\n",
    "    index=[\"Mean (Top 5 rankers)\", \"Mean\", \"Std\"],\n",
    "    columns=[\"y_train_updated\", \"y_test_updated\"],\n",
    "    data=[\n",
    "        [round(y_train_updated[y_train_updated<=5].mean(), 4),\n",
    "         round(y_test_updated[y_test_updated<=5].mean(), 4)],\n",
    "        [round(y_train_updated.mean(), 4),\n",
    "         round(y_test_updated.mean(), 4)],\n",
    "        [round(y_train_updated.std(), 4),\n",
    "         round(y_test_updated.std(), 4)]\n",
    "    ]\n",
    ")\n",
    "print(\"(Updated) Ground truth stats:\")\n",
    "print(stats_df_updated,\"\\n\")\n",
    "\n",
    "print(\"(Updated) Train stats:\")\n",
    "xgb_train_result_updated = get_rank_predictions(\n",
    "    X_train_updated, y_train_updated, xgb_ranker, target_column=\"rank\", target=\"candidates\")\n",
    "\n",
    "print(\"(Updated) Test stats:\")\n",
    "xgb_test_result_updated = get_rank_predictions(\n",
    "    X_test_updated, y_test_updated, xgb_ranker, target_column=\"rank\", target=\"candidates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Updated) Ground truth stats:\n",
      "                      y_train_updated  y_test_updated\n",
      "Mean (Top 5 rankers)           3.2632          3.0000\n",
      "Mean                          15.5000          6.2500\n",
      "Std                           10.5795          3.9749 \n",
      "\n",
      "(Updated) Train stats:\n",
      "Mean rank of top 5 candidates based on predictions: 3.8947\n",
      "Mean rank of all candidates based on predictions: 14.8226\n",
      "Std rank of all candidates based on predictions: 10.5419\n",
      "Mean absolute difference between each pair of rank and predicted rank: 2.7419\n",
      "    rank  pred_rank  abs_diff\n",
      "id                           \n",
      "60   4.0        1.0       3.0\n",
      "36   4.0        1.0       3.0\n",
      "49   4.0        1.0       3.0\n",
      "6    4.0        1.0       3.0\n",
      "24   4.0        1.0       3.0 \n",
      "\n",
      "(Updated) Test stats:\n",
      "Mean rank of top 5 candidates based on predictions: 4.0\n",
      "Mean rank of all candidates based on predictions: 6.25\n",
      "Std rank of all candidates based on predictions: 3.3961\n",
      "Mean absolute difference between each pair of rank and predicted rank: 2.0\n",
      "    rank  pred_rank  abs_diff\n",
      "id                           \n",
      "73   1.0        1.0       0.0\n",
      "66   4.0        2.0       2.0\n",
      "99   2.0        3.0       1.0\n",
      "50   1.0        4.0       3.0\n",
      "9    5.0        4.0       1.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lgbm_ranker.fit(\n",
    "    X=X_train_updated,\n",
    "    y=y_train_updated,\n",
    "    group=y_train_updated.value_counts(),\n",
    "    eval_set=[(X_test_updated, y_test_updated)],\n",
    "    eval_group=[y_test_updated.value_counts()],\n",
    "    verbose=-1\n",
    "    )\n",
    "\n",
    "print(\"(Updated) Ground truth stats:\")\n",
    "print(stats_df_updated,\"\\n\")\n",
    "\n",
    "print(\"(Updated) Train stats:\")\n",
    "lgbm_train_result_updated = get_rank_predictions(\n",
    "    X_train_updated, y_train_updated, lgbm_ranker, target_column=\"rank\", target=\"candidates\")\n",
    "\n",
    "print(\"(Updated) Test stats:\")\n",
    "lgbm_test_result_updated = get_rank_predictions(\n",
    "    X_test_updated, y_test_updated, lgbm_ranker, target_column=\"rank\", target=\"candidates\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Evaluate results.\n",
    "\n",
    "Collate and re-arrange statistics for easier evaluation of the results and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_train</th>\n",
       "      <th>y_train_updated</th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_test_updated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mean (Top 5 rankers)</th>\n",
       "      <td>2.7368</td>\n",
       "      <td>3.2632</td>\n",
       "      <td>3.3750</td>\n",
       "      <td>3.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean</th>\n",
       "      <td>15.0323</td>\n",
       "      <td>15.5000</td>\n",
       "      <td>6.1250</td>\n",
       "      <td>6.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Std</th>\n",
       "      <td>10.4880</td>\n",
       "      <td>10.5795</td>\n",
       "      <td>3.2838</td>\n",
       "      <td>3.9749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      y_train  y_train_updated  y_test  y_test_updated\n",
       "Mean (Top 5 rankers)   2.7368           3.2632  3.3750          3.0000\n",
       "Mean                  15.0323          15.5000  6.1250          6.2500\n",
       "Std                   10.4880          10.5795  3.2838          3.9749"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_df_concat = pd.concat([stats_df, stats_df_updated], axis=1)\n",
    "stats_df_concat = stats_df_concat.iloc[:, [0, 2, 1, 3]] # re-arrange columns\n",
    "stats_df_concat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions on top 5 candidates were similar before and after the starring of ideal candidates. In other words, the ranking models can handle the update of ranks based on stars, i.e. the new binary column 'star'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall mean statistics:\n",
      "           xgb_train  xgb_train_updated  xgb_test  xgb_test_updated  \\\n",
      "rank         15.0323            15.5000     6.125            6.2500   \n",
      "pred_rank    15.0806            15.8387     6.125            7.1875   \n",
      "abs_diff      2.9194             2.9194     1.625            1.9375   \n",
      "\n",
      "           lgbm_train  lgbm_train_updated  lgbm_test  lgbm_test_updated  \n",
      "rank          15.0323             15.5000      6.125               6.25  \n",
      "pred_rank     15.1129             14.8226      6.250               6.25  \n",
      "abs_diff       2.3387              2.7419      1.375               2.00  \n"
     ]
    }
   ],
   "source": [
    "overall_mean_dict = {}\n",
    "\n",
    "overall_mean_dict[\"xgb_train\"] = round(xgb_train_result.mean(), 4).to_dict()\n",
    "overall_mean_dict[\"xgb_train_updated\"] = round(xgb_train_result_updated.mean(), 4).to_dict()\n",
    "overall_mean_dict[\"xgb_test\"] = round(xgb_test_result.mean(), 4).to_dict()\n",
    "overall_mean_dict[\"xgb_test_updated\"] = round(xgb_test_result_updated.mean(), 4).to_dict()\n",
    "\n",
    "overall_mean_dict[\"lgbm_train\"] = round(lgbm_train_result.mean(), 4).to_dict()\n",
    "overall_mean_dict[\"lgbm_train_updated\"] = round(lgbm_train_result_updated.mean(), 4).to_dict()\n",
    "overall_mean_dict[\"lgbm_test\"] = round(lgbm_test_result.mean(), 4).to_dict()\n",
    "overall_mean_dict[\"lgbm_test_updated\"] = round(lgbm_test_result_updated.mean(), 4).to_dict()\n",
    "\n",
    "overall_mean_df = pd.DataFrame(overall_mean_dict)\n",
    "print(f\"Overall mean statistics:\\n{overall_mean_df}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 mean statistics:\n",
      "           xgb_train  xgb_train_updated  xgb_test  xgb_test_updated  \\\n",
      "rank          2.7368             3.2632     3.375             3.000   \n",
      "pred_rank     5.1579             3.8947     3.125             4.625   \n",
      "abs_diff      2.4211             2.1053     0.750             1.625   \n",
      "\n",
      "           lgbm_train  lgbm_train_updated  lgbm_test  lgbm_test_updated  \n",
      "rank           2.7368              3.2632      3.375               3.00  \n",
      "pred_rank      5.0526              3.8947      3.500               4.00  \n",
      "abs_diff       2.3158              2.9474      0.625               2.25  \n"
     ]
    }
   ],
   "source": [
    "top5_mean_dict = {}\n",
    "\n",
    "top5_mean_dict[\"xgb_train\"] = round(xgb_train_result[xgb_train_result[\"rank\"]<=5].mean(), 4).to_dict()\n",
    "top5_mean_dict[\"xgb_train_updated\"] = round(xgb_train_result_updated[xgb_train_result_updated[\"rank\"]<=5].mean(), 4).to_dict()\n",
    "top5_mean_dict[\"xgb_test\"] = round(xgb_test_result[xgb_test_result[\"rank\"]<=5].mean(), 4).to_dict()\n",
    "top5_mean_dict[\"xgb_test_updated\"] = round(xgb_test_result_updated[xgb_test_result_updated[\"rank\"]<=5].mean(), 4).to_dict()\n",
    "\n",
    "top5_mean_dict[\"lgbm_train\"] = round(lgbm_train_result[lgbm_train_result[\"rank\"]<=5].mean(), 4).to_dict()\n",
    "top5_mean_dict[\"lgbm_train_updated\"] = round(lgbm_train_result_updated[lgbm_train_result_updated[\"rank\"]<=5].mean(), 4).to_dict()\n",
    "top5_mean_dict[\"lgbm_test\"] = round(lgbm_test_result[lgbm_test_result[\"rank\"]<=5].mean(), 4).to_dict()\n",
    "top5_mean_dict[\"lgbm_test_updated\"] = round(lgbm_test_result_updated[lgbm_test_result_updated[\"rank\"]<=5].mean(), 4).to_dict()\n",
    "\n",
    "top5_mean_df = pd.DataFrame(top5_mean_dict)\n",
    "print(f\"Top 5 mean statistics:\\n{top5_mean_df}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the overall and top 5 mean statistics, the performance of each model in terms of the mean absolute difference between real ranks and predicted ranks could vary based on the particular selection of ideal candidates, but on the whole both models appear to be able to handle the re-ranking of candidates - the performances didn't dramatically decrease (or increase) after the re-training based on the starring of ideal candidates."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Save the models for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Conclusion\n",
    "\n",
    "goal and success metrics\n",
    "was able to get fitness scores using various word embedding techniques and use them to train LTR models.\n",
    "evaluated the model predictions - looked at means, sts, whole candidates, only top 5, etc. \n",
    "the process of re-ranking (or starring) candidates was built into the machine learning pipeline and the models were able to handle updated ranks and produce reasonable predictions based on the updated ranks.\n",
    "\n",
    "\n",
    "other objectives\n",
    "- We are interested in a robust algorithm, tell us how your solution works and show us how your ranking gets better with each starring action.\n",
    "my solutions do not soley rely on a single ranking algorithm or word embedding technique.\n",
    "\n",
    "the fit column is calculated based on 5 different fitness metrics. each of those used the cosine similarity between the vector of job title and the vector of keywords for each candidate, where job titles that are highly related to keywords (e.g. human resources) will get higher cosine similarity scores.\n",
    "\n",
    "as for rankers, XGB ranker and lgbm ranker can complement each other - can use both models for selecting top candidates to reduce the cahnge of missing high potential candidates.\n",
    "\n",
    "- How can we filter out candidates which in the first place should not be in this list?\n",
    "based on how fitness scores were calculated in this notebook, candidates with a fitness score of 0 from the fit column can be dropped. If no candidates have a fitness score of 0, can drop candidates with at least 1 '0 fitness score' from any of the fit columns such as fit_tfidf, fit_glove - this information is stored in the has_zero_scores column.\n",
    "\n",
    "looking at the job titles of those candidates, they did look irrelevant.\n",
    "\n",
    "- Can we determine a cut-off point that would work for other roles without losing high potential candidates?\n",
    "based on how fitness scores were calculated in this notebook, candidates with a fitness score of 0 from the fit column can be dropped. If no candidates have a fitness score of 0, can drop candidates with at least 1 '0 fitness score' from any of the fit columns such as fit_tfidf, fit_glove - this information is stored in the has_zero_scores column.\n",
    "\n",
    "- Do you have any ideas that we should explore so that we can even automate this procedure to prevent human bias?\n",
    "\n",
    "\n",
    "10-1. Learning To Rank (LTR) models - XGB Ranker, LGBM Ranker\n",
    "results - the model performance, etc. mention goals and success metrics mentioned in the project description (README).\n",
    "\n",
    "10-2. Word embeddings and vectorizations - TF-IDF, Tokenizer, GloVe, Word2Vec, FastText\n",
    "\n",
    "\n",
    "as a whole, think of the notebook as a story. when a person (interviewer) reads the notebook, they should be able to get what you are trying to do and present.\n",
    "\n",
    "send an email if I submit before next session.\n",
    "\n",
    "\n",
    "\n",
    "10-5. Application\n",
    "the solution can be used in ## situations, business cases, etc.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a4868653bb6f8972e87e4c446ab8a445a15b25dedb8594cc74c480f8152ea86a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
