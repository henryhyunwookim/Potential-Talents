{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Table of Content</b>\n",
    "\n",
    "0. Import functions\n",
    "\n",
    "1. Load and explore data\n",
    "\n",
    "2. Pre-process job titles\n",
    "\n",
    "3. Get fitness scores\n",
    "\n",
    "    3-1. TF-IDF\n",
    "   \n",
    "    3-2. TensorFlow Tokenizer\n",
    "\n",
    "    3-3. GloVe\n",
    "   \n",
    "    3-4. Word2Vec\n",
    "   \n",
    "    3-5. FastText\n",
    "\n",
    "4. Scale and evaluate fitness scores\n",
    "\n",
    "5. Drop irrelevant candidates\n",
    "\n",
    "6. Present initial list of top candidates\n",
    "\n",
    "7. Prepare train and test data\n",
    "\n",
    "8. Train ranking models\n",
    "   \n",
    "    8-1. XGBoost Ranker\n",
    "   \n",
    "    8-2. LGBM Ranker\n",
    "\n",
    "9. Star ideal candidates\n",
    "\n",
    "    9-1. Get ids of ideal candidates to star\n",
    "\n",
    "    9-2. Create a copy of train and test data for re-ranking candidates based on stars\n",
    "\n",
    "    9-3. Add a binary feature 'star' to training features\n",
    "\n",
    "10. Re-rank all candidates\n",
    "\n",
    "11. Re-train ranking models based on updated ranks and training features\n",
    "\n",
    "12. Evaluate results\n",
    "\n",
    "13. Save models for later use\n",
    "\n",
    "14. Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>0. Import functions</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.load import load_data\n",
    "from utils.split import split_data\n",
    "from utils.transform import add_zero_score_col, update_ranks\n",
    "from utils.process_text import get_relevant_terms, update_str_col, process_text, convert_words_to_vectors, get_word_vectors\n",
    "from utils.predict import get_stats\n",
    "from utils.evaluate import cosine_similarity, get_mean_stats\n",
    "from utils.save import save_model\n",
    "\n",
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "from xgboost import XGBRanker\n",
    "from lightgbm import LGBMRanker\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>1. Load and explore data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 104 entries, 0 to 103\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   id          104 non-null    int64  \n",
      " 1   job_title   104 non-null    object \n",
      " 2   location    104 non-null    object \n",
      " 3   connection  104 non-null    object \n",
      " 4   fit         0 non-null      float64\n",
      "dtypes: float64(1), int64(1), object(3)\n",
      "memory usage: 4.2+ KB\n",
      "None \n",
      "\n",
      "               id  fit\n",
      "count  104.000000  0.0\n",
      "mean    52.500000  NaN\n",
      "std     30.166206  NaN\n",
      "min      1.000000  NaN\n",
      "25%     26.750000  NaN\n",
      "50%     52.500000  NaN\n",
      "75%     78.250000  NaN\n",
      "max    104.000000  NaN \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>connection</th>\n",
       "      <th>fit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2019 C.T. Bauer College of Business Graduate (...</td>\n",
       "      <td>Houston, Texas</td>\n",
       "      <td>85</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Native English Teacher at EPIK (English Progra...</td>\n",
       "      <td>Kanada</td>\n",
       "      <td>500+</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Aspiring Human Resources Professional</td>\n",
       "      <td>Raleigh-Durham, North Carolina Area</td>\n",
       "      <td>44</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>People Development Coordinator at Ryan</td>\n",
       "      <td>Denton, Texas</td>\n",
       "      <td>500+</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Advisory Board Member at Celal Bayar University</td>\n",
       "      <td>İzmir, Türkiye</td>\n",
       "      <td>500+</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                          job_title  \\\n",
       "0   1  2019 C.T. Bauer College of Business Graduate (...   \n",
       "1   2  Native English Teacher at EPIK (English Progra...   \n",
       "2   3              Aspiring Human Resources Professional   \n",
       "3   4             People Development Coordinator at Ryan   \n",
       "4   5    Advisory Board Member at Celal Bayar University   \n",
       "\n",
       "                              location connection  fit  \n",
       "0                       Houston, Texas         85  NaN  \n",
       "1                               Kanada      500+   NaN  \n",
       "2  Raleigh-Durham, North Carolina Area         44  NaN  \n",
       "3                        Denton, Texas      500+   NaN  \n",
       "4                       İzmir, Türkiye      500+   NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_data(file_name=\"potential-talents.xlsx\", folder_name=\"data\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ProfileReport(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The id column is just an index column that would not be relevant to the fitness of any roles.\n",
    "Although the job_title and location columns are highly correlated, the job_title column seems to be the only relevant column in determining the fitness of a particular role based on the column values and information we have about the requirements.\n",
    "\n",
    "Therefore, only the job_title column will be used in the ranking procedures. Having said that, the other columns will still be returned in the result so that the user (i.e. the client) can have the full information about each of the relevant candidates.\n",
    "The fit column will be filled with a fitness score for each candidate in the next steps."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>2. Pre-process job titles</b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert human resources-related terms in a way that job titles containing those terms will have better fitness scores. That is, those job titles might end up having a fitness score of 0 without conversion because for instance \"HR\" and \"Human Resources\" would be considered to have nothing in common by most algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SPHR', 'CHRO,', 'HRIS', 'GPHR', 'HR']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hr_words = get_relevant_terms(word_list=data['job_title'], term=\"HR\")\n",
    "hr_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_terms_dict = {'CHRO,': 'Chief Human Resources Officer,',\n",
    "                'GPHR': 'Global Professional in Human Resources',\n",
    "                'SPHR': 'Senior Professional in Human Resources',\n",
    "                'HR': 'Human Resources',\n",
    "                'HRIS': 'Human Resources Information System',\n",
    "                'People': 'Human'} # this is for job titles like 'People Development Coordinator'.\n",
    "\n",
    "data = update_str_col(dataframe=data, column='job_title', mapping_dict=hr_terms_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar conversions can be done for terms like \"staff*\", \"employ*\", but we will leave the decision to domain experts later and for now only convert terms that specifically include \"HR\" as above."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>3. Get fitness scores</b>\n",
    "\n",
    "Vectorize job titles and keywords using different word embedding techniques and calculate cosine similarity between the vectors. Higher cosine similarity would mean the job titles and keywords are more closely related."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>3-1. TF-IDF</b>\n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) quantifies how relevant a word is to a document in a collection of documents or corpus.\n",
    "* Word < Document < Corpus (Collection of Documents)\n",
    "\n",
    "For instance, if a word appears in a document many times but also appears across different documents many times, then the word will have a low TF-IDF score, meaning the word is less important to that particular document since the word is just so common in any other documents. An example of a term like this would be \"the\", which is not very meaningful to any particular document.\n",
    "\n",
    "Whereas if a word appears in a document many times but it rarely appears in other documents, then it would mean that the word is important in that particular document. To put it differently, we consider the word and the document are highly related.\n",
    "\n",
    "Another thing to note is that it is often important to pre-process text data such as removing stop words, lemmatize, etc. before vectorizing through TF-IDF in order to get better (or more useful) results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_args = {'strip_accents':'unicode',\n",
    "              'lowercase':True,\n",
    "              'stop_words':'english',\n",
    "              'ngram_range':(1,3)}\n",
    "tfidf_vectorizer = TfidfVectorizer(**tfidf_args)\n",
    "\n",
    "job_title_processed_tfidf = data['job_title'].apply(\n",
    "    process_text,\n",
    "    remove_stopwords=True,\n",
    "    lemmatize=True,\n",
    "    stem=True\n",
    ")\n",
    "\n",
    "keywords = [\"Aspiring human resources\", \"seeking human resources\"]\n",
    "keywords_processed_tfidf = [process_text(keyword) for keyword in keywords]\n",
    "\n",
    "data['fit_tfidf'] = cosine_similarity(tfidf_vectorizer.fit_transform(job_title_processed_tfidf),\n",
    "                                      tfidf_vectorizer.transform(keywords_processed_tfidf)).sum(axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For other vectorizers later on, still remove stopwords since stopwords do not add any values or meanings. However, do not lemmatize or stem because such pre-processing could result in worse results when using algorithms that have a pre-defined vocabulary, dictionary, or corpus. This is because they would not be able to provide meaningful word embedding when a lemma or stem is not found in the collection of words that they refer to for word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title_processed = data['job_title'].apply(\n",
    "    process_text,\n",
    "    remove_stopwords=True,\n",
    "    lemmatize=False,\n",
    "    stem=False\n",
    ")\n",
    "keywords_processed = [process_text(\n",
    "    keyword,\n",
    "    remove_stopwords=True,\n",
    "    lemmatize=False,\n",
    "    stem=False\n",
    "    ) for keyword in keywords]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>3-2. TensorFlow Tokenizer</b>\n",
    "\n",
    "Tokenization is the process of breaking up a string into tokens. Commonly, these tokens are words, numbers, and/or punctuation.\n",
    "\n",
    "TensorFlow's Tokenizer class allows users to vectorize a text corpus by turning each text into either a sequence of integers (each integer being the index of a token in a dictionary) or into a vector where the coefficient for each token could be binary, based on word count, based on TF-IDF, etc.\n",
    "\n",
    "By default, all punctuation is removed, turning the texts into space-separated sequences of words (words may include the ' character). These sequences are then split into lists of tokens. They will then be indexed or vectorized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(job_title_processed) # fit_on_texts updates internal vocabulary based on a list of texts; similar to tf-idf.\n",
    "data['fit_keras_tokenizer'] = cosine_similarity(tokenizer.texts_to_matrix(job_title_processed),\n",
    "                                                tokenizer.texts_to_matrix(keywords_processed)).sum(axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on, we are going to use Gensim.<br>\n",
    "Gensim is an open-source library for unsupervised topic modeling, document indexing, retrieval by similarity, and other natural language processing functionalities. Available modules from the library include Word2Vec, glove2word2vec, and fasttext, which will be used for word embeddings (or vectorization) in this project.\n",
    "\n",
    "##### <b>3-3. GloVe</b>\n",
    "\n",
    "GloVe stands for Global Vectors for word representation. It is an unsupervised learning algorithm that generates word embeddings by aggregating global word co-occurrence matrices from a given corpus. That is, GloVe allows users to take a corpus of text and transform each word in the corpus into a position in a high-dimensional space, and this is achieved by mapping words (from the corpus) into a space (or dimensions) where the distance between words is related to semantic similarity.\n",
    "\n",
    "* glove2word2vec allows users to convert GloVe vectors into the word2vec. Both files are presented in text format and are almost identical except that word2vec includes the number of vectors and its dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove file source: https://nlp.stanford.edu/projects/glove/\n",
    "word2vec_file = get_tmpfile('word2vec.6B.50d.txt') # Create a temp file\n",
    "glove2word2vec('data/glove/glove.6B.50d.txt', word2vec_file) # Save glove2word2vec into the temp file\n",
    "glove_vectors = KeyedVectors.load_word2vec_format(word2vec_file) # Load the glove2word2vec from the teamp file\n",
    "glove_dimension = 50\n",
    "\n",
    "# Transform job titles and keywords into glove vectors\n",
    "glove_vectors_job_title = convert_words_to_vectors(job_title_processed, glove_vectors, glove_dimension)\n",
    "glove_vectors_keywords = convert_words_to_vectors(keywords_processed, glove_vectors, glove_dimension)\n",
    "data['fit_glove'] = cosine_similarity(glove_vectors_job_title, glove_vectors_keywords).sum(axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>3-4. Word2Vec</b>\n",
    "\n",
    "Similar to GloVe, Word2Vec is an unsupervised learning algorithm that creates a distributed representation of words into numerical vectors. Simply put, it converts text (i.e. a collection of words) into numerical vectors in a high-dimensional space. Those vectorized words can capture semantics and relationships among words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(sentences=job_title_processed.apply(lambda x: [word.lower() for word in x.split()]))\n",
    "word2vec_dimension = word2vec.vector_size\n",
    "\n",
    "# Transform job titles and keywords into word2vec vectors\n",
    "word2vec_job_title = convert_words_to_vectors(job_title_processed, word2vec, word2vec_dimension)\n",
    "word2vec_keywords = convert_words_to_vectors(keywords_processed, word2vec, word2vec_dimension)\n",
    "data['fit_word2vec'] = cosine_similarity(word2vec_job_title, word2vec_keywords).sum(axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>3-5. FastText</b>\n",
    "\n",
    "One of the main disadvantages of Word2Vec and GloVe embeddings is that they are unable to encode unknown or out-of-vocabulary words. To deal with this problem, FastText was created by then Facebook (now Meta) in 2015. FastText takes into account the internal structure of words while learning word representations, which allows it to provide better embeddings for morphologically rich languages, for words that rarely occur, or even for made-up words. In other words, FastText word vectors are built from vectors of substrings of characters contained in it, and this allows it to produce vectors for any word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext = FastText(sentences=job_title_processed.apply(lambda x: [word.lower() for word in x.split()]))\n",
    "fasttext_dimension = fasttext.vector_size\n",
    "\n",
    "# Transform job titles and keywords into fasttext vectors\n",
    "fasttext_job_title = convert_words_to_vectors(job_title_processed, fasttext, fasttext_dimension)\n",
    "fasttext_keywords = convert_words_to_vectors(keywords_processed, fasttext, fasttext_dimension)\n",
    "data['fit_fasttext'] = cosine_similarity(fasttext_job_title, fasttext_keywords).sum(axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>4. Scale and evaluate fitness scores</b>\n",
    "\n",
    "Transform fit scores so that different fit scores will have the same range between 0 and 1.<br>\n",
    "This is for easier comparisons among different fit scores created by various algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>fit</th>\n",
       "      <th>fit_tfidf</th>\n",
       "      <th>fit_keras_tokenizer</th>\n",
       "      <th>fit_glove</th>\n",
       "      <th>fit_word2vec</th>\n",
       "      <th>fit_fasttext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>104.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>104.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>52.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.328938</td>\n",
       "      <td>0.541808</td>\n",
       "      <td>0.604935</td>\n",
       "      <td>0.597979</td>\n",
       "      <td>0.586781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>30.166206</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.315271</td>\n",
       "      <td>0.359278</td>\n",
       "      <td>0.313345</td>\n",
       "      <td>0.307347</td>\n",
       "      <td>0.290476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>26.750000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.057644</td>\n",
       "      <td>0.106066</td>\n",
       "      <td>0.286359</td>\n",
       "      <td>0.338444</td>\n",
       "      <td>0.422400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>52.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.253802</td>\n",
       "      <td>0.642826</td>\n",
       "      <td>0.664575</td>\n",
       "      <td>0.721075</td>\n",
       "      <td>0.684321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>78.250000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.497634</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.864163</td>\n",
       "      <td>0.815329</td>\n",
       "      <td>0.776623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>104.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id  fit   fit_tfidf  fit_keras_tokenizer   fit_glove  \\\n",
       "count  104.000000  0.0  104.000000           104.000000  104.000000   \n",
       "mean    52.500000  NaN    0.328938             0.541808    0.604935   \n",
       "std     30.166206  NaN    0.315271             0.359278    0.313345   \n",
       "min      1.000000  NaN    0.000000             0.000000    0.000000   \n",
       "25%     26.750000  NaN    0.057644             0.106066    0.286359   \n",
       "50%     52.500000  NaN    0.253802             0.642826    0.664575   \n",
       "75%     78.250000  NaN    0.497634             0.800000    0.864163   \n",
       "max    104.000000  NaN    1.000000             1.000000    1.000000   \n",
       "\n",
       "       fit_word2vec  fit_fasttext  \n",
       "count    104.000000    104.000000  \n",
       "mean       0.597979      0.586781  \n",
       "std        0.307347      0.290476  \n",
       "min        0.000000      0.000000  \n",
       "25%        0.338444      0.422400  \n",
       "50%        0.721075      0.684321  \n",
       "75%        0.815329      0.776623  \n",
       "max        1.000000      1.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minmax_scaler = MinMaxScaler()\n",
    "fit_columns = [col for col in data.columns if \"fit_\" in col]\n",
    "data[fit_columns] = minmax_scaler.fit_transform(data[fit_columns])\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that each of the fit columns (there are 5 'fit_' columns in total) has a scale of 0-1, sum all fit scores so that the 'fit' column will have a score between 0 and 5 for each candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['fit'] = data[fit_columns].sum(axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No candidate has a fitness score of 0 as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: job_title, dtype: int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['fit']==0].job_title.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the job titles of the candidates who got the highest fitness scores, they indeed look very relevant to what we are looking for (i.e. \"Aspiring human resources\" and \"seeking human resources\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>connection</th>\n",
       "      <th>fit</th>\n",
       "      <th>fit_tfidf</th>\n",
       "      <th>fit_keras_tokenizer</th>\n",
       "      <th>fit_glove</th>\n",
       "      <th>fit_word2vec</th>\n",
       "      <th>fit_fasttext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Seeking Human Resources Opportunities</td>\n",
       "      <td>Chicago, Illinois</td>\n",
       "      <td>390</td>\n",
       "      <td>4.828453</td>\n",
       "      <td>0.921024</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.973559</td>\n",
       "      <td>0.933869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>Seeking Human Resources Opportunities</td>\n",
       "      <td>Chicago, Illinois</td>\n",
       "      <td>390</td>\n",
       "      <td>4.828453</td>\n",
       "      <td>0.921024</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.973559</td>\n",
       "      <td>0.933869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>Seeking Human Resources Position</td>\n",
       "      <td>Las Vegas, Nevada Area</td>\n",
       "      <td>48</td>\n",
       "      <td>4.762808</td>\n",
       "      <td>0.913385</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.953443</td>\n",
       "      <td>0.973559</td>\n",
       "      <td>0.922421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>Aspiring Human Resources Professional</td>\n",
       "      <td>Raleigh-Durham, North Carolina Area</td>\n",
       "      <td>44</td>\n",
       "      <td>4.693111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.920193</td>\n",
       "      <td>0.815329</td>\n",
       "      <td>0.957589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Aspiring Human Resources Professional</td>\n",
       "      <td>Raleigh-Durham, North Carolina Area</td>\n",
       "      <td>44</td>\n",
       "      <td>4.693111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.920193</td>\n",
       "      <td>0.815329</td>\n",
       "      <td>0.957589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                              job_title  \\\n",
       "27  28  Seeking Human Resources Opportunities   \n",
       "29  30  Seeking Human Resources Opportunities   \n",
       "98  99       Seeking Human Resources Position   \n",
       "32  33  Aspiring Human Resources Professional   \n",
       "2    3  Aspiring Human Resources Professional   \n",
       "\n",
       "                               location connection       fit  fit_tfidf  \\\n",
       "27                    Chicago, Illinois        390  4.828453   0.921024   \n",
       "29                    Chicago, Illinois        390  4.828453   0.921024   \n",
       "98               Las Vegas, Nevada Area         48  4.762808   0.913385   \n",
       "32  Raleigh-Durham, North Carolina Area         44  4.693111   1.000000   \n",
       "2   Raleigh-Durham, North Carolina Area         44  4.693111   1.000000   \n",
       "\n",
       "    fit_keras_tokenizer  fit_glove  fit_word2vec  fit_fasttext  \n",
       "27                  1.0   1.000000      0.973559      0.933869  \n",
       "29                  1.0   1.000000      0.973559      0.933869  \n",
       "98                  1.0   0.953443      0.973559      0.922421  \n",
       "32                  1.0   0.920193      0.815329      0.957589  \n",
       "2                   1.0   0.920193      0.815329      0.957589  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sort_values('fit', ascending=False).head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, the job titles with the lowest fitness scores do seem irrelevant - they don't include any terms related to human resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>connection</th>\n",
       "      <th>fit</th>\n",
       "      <th>fit_tfidf</th>\n",
       "      <th>fit_keras_tokenizer</th>\n",
       "      <th>fit_glove</th>\n",
       "      <th>fit_word2vec</th>\n",
       "      <th>fit_fasttext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>Advisory Board Member at Celal Bayar University</td>\n",
       "      <td>İzmir, Türkiye</td>\n",
       "      <td>500+</td>\n",
       "      <td>0.235728</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.066091</td>\n",
       "      <td>0.169637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>48</td>\n",
       "      <td>Advisory Board Member at Celal Bayar University</td>\n",
       "      <td>İzmir, Türkiye</td>\n",
       "      <td>500+</td>\n",
       "      <td>0.235728</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.066091</td>\n",
       "      <td>0.169637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Advisory Board Member at Celal Bayar University</td>\n",
       "      <td>İzmir, Türkiye</td>\n",
       "      <td>500+</td>\n",
       "      <td>0.235728</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.066091</td>\n",
       "      <td>0.169637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>Advisory Board Member at Celal Bayar University</td>\n",
       "      <td>İzmir, Türkiye</td>\n",
       "      <td>500+</td>\n",
       "      <td>0.235728</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.066091</td>\n",
       "      <td>0.169637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>87</td>\n",
       "      <td>Bachelor of Science in Biology from Victoria U...</td>\n",
       "      <td>Baltimore, Maryland</td>\n",
       "      <td>40</td>\n",
       "      <td>0.241118</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05825</td>\n",
       "      <td>0.066091</td>\n",
       "      <td>0.116777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                          job_title  \\\n",
       "34  35    Advisory Board Member at Celal Bayar University   \n",
       "47  48    Advisory Board Member at Celal Bayar University   \n",
       "4    5    Advisory Board Member at Celal Bayar University   \n",
       "22  23    Advisory Board Member at Celal Bayar University   \n",
       "86  87  Bachelor of Science in Biology from Victoria U...   \n",
       "\n",
       "               location connection       fit  fit_tfidf  fit_keras_tokenizer  \\\n",
       "34       İzmir, Türkiye      500+   0.235728        0.0                  0.0   \n",
       "47       İzmir, Türkiye      500+   0.235728        0.0                  0.0   \n",
       "4        İzmir, Türkiye      500+   0.235728        0.0                  0.0   \n",
       "22       İzmir, Türkiye      500+   0.235728        0.0                  0.0   \n",
       "86  Baltimore, Maryland         40  0.241118        0.0                  0.0   \n",
       "\n",
       "    fit_glove  fit_word2vec  fit_fasttext  \n",
       "34    0.00000      0.066091      0.169637  \n",
       "47    0.00000      0.066091      0.169637  \n",
       "4     0.00000      0.066091      0.169637  \n",
       "22    0.00000      0.066091      0.169637  \n",
       "86    0.05825      0.066091      0.116777  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sort_values('fit', ascending=True).head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>5. Drop irrelevant candidates</b>\n",
    "\n",
    "As we have confirmed that our scoring methods (i.e. the fitness scores) work as expected without dramatic deviation, we can safely drop the most irrelevant candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>fit</th>\n",
       "      <th>fit_tfidf</th>\n",
       "      <th>fit_keras_tokenizer</th>\n",
       "      <th>fit_glove</th>\n",
       "      <th>fit_word2vec</th>\n",
       "      <th>fit_fasttext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>104.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>104.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>52.500000</td>\n",
       "      <td>2.660440</td>\n",
       "      <td>0.328938</td>\n",
       "      <td>0.541808</td>\n",
       "      <td>0.604935</td>\n",
       "      <td>0.597979</td>\n",
       "      <td>0.586781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>30.166206</td>\n",
       "      <td>1.474100</td>\n",
       "      <td>0.315271</td>\n",
       "      <td>0.359278</td>\n",
       "      <td>0.313345</td>\n",
       "      <td>0.307347</td>\n",
       "      <td>0.290476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.235728</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>26.750000</td>\n",
       "      <td>1.590360</td>\n",
       "      <td>0.057644</td>\n",
       "      <td>0.106066</td>\n",
       "      <td>0.286359</td>\n",
       "      <td>0.338444</td>\n",
       "      <td>0.422400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>52.500000</td>\n",
       "      <td>2.952384</td>\n",
       "      <td>0.253802</td>\n",
       "      <td>0.642826</td>\n",
       "      <td>0.664575</td>\n",
       "      <td>0.721075</td>\n",
       "      <td>0.684321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>78.250000</td>\n",
       "      <td>3.540943</td>\n",
       "      <td>0.497634</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.864163</td>\n",
       "      <td>0.815329</td>\n",
       "      <td>0.776623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>104.000000</td>\n",
       "      <td>4.828453</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id         fit   fit_tfidf  fit_keras_tokenizer   fit_glove  \\\n",
       "count  104.000000  104.000000  104.000000           104.000000  104.000000   \n",
       "mean    52.500000    2.660440    0.328938             0.541808    0.604935   \n",
       "std     30.166206    1.474100    0.315271             0.359278    0.313345   \n",
       "min      1.000000    0.235728    0.000000             0.000000    0.000000   \n",
       "25%     26.750000    1.590360    0.057644             0.106066    0.286359   \n",
       "50%     52.500000    2.952384    0.253802             0.642826    0.664575   \n",
       "75%     78.250000    3.540943    0.497634             0.800000    0.864163   \n",
       "max    104.000000    4.828453    1.000000             1.000000    1.000000   \n",
       "\n",
       "       fit_word2vec  fit_fasttext  \n",
       "count    104.000000    104.000000  \n",
       "mean       0.597979      0.586781  \n",
       "std        0.307347      0.290476  \n",
       "min        0.000000      0.000000  \n",
       "25%        0.338444      0.422400  \n",
       "50%        0.721075      0.684321  \n",
       "75%        0.815329      0.776623  \n",
       "max        1.000000      1.000000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the min fitness scores, no candidate has a 'fit' score of 0 (the min score of the 'fit' column is 0.357239) although the minimum fitness score of each of the 'fit_' columns is 0.\n",
    "\n",
    "To filter candidates with at least 1 zero fitness score, add a new column 'has_zero_scores' to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Native English Teacher at EPIK (English Program in Korea)',\n",
       "       'Advisory Board Member at Celal Bayar University',\n",
       "       'Student at Chapman University',\n",
       "       'Junior MES Engineer| Information Systems',\n",
       "       'RRP Brand Portfolio Executive at JTI (Japan Tobacco International)',\n",
       "       'Information Systems Specialist and Programmer with a love for data and organization.',\n",
       "       'Bachelor of Science in Biology from Victoria University of Wellington',\n",
       "       'Undergraduate Research Assistant at Styczynski Lab',\n",
       "       'Lead Official at Western Illinois University',\n",
       "       'Admissions Representative at Community medical center long beach',\n",
       "       'Student at Westfield State University',\n",
       "       'Student at Indiana University Kokomo - Business Management - Retail Manager at Delphi Hardware and Paint',\n",
       "       'Student', 'Business Intelligence and Analytics at Travelers',\n",
       "       'Always set them up for Success',\n",
       "       'Director Of Administration at Excellence Logging'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = add_zero_score_col(data, fit_columns)\n",
    "data[data['has_zero_scores'] == 1].job_title.unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These job titles indeed seem irrelevant to our keywords, \"Aspiring human resources\" and \"seeking human resources\".\n",
    "\n",
    "To confirm if we can drop candidates with these job titles, let's look at the candidates with the worst fitness scores, excluding candidates with at least 1 zero fitness score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_tfidf least fit job title 1: Human Development Coordinator at Ryan\n",
      "fit_tfidf least fit job title 2: Human Development Coordinator at Ryan\n",
      "fit_tfidf least fit job title 3: Human Development Coordinator at Ryan\n",
      "fit_tfidf least fit job title 4: Human Development Coordinator at Ryan\n",
      "fit_tfidf least fit job title 5: Human Development Coordinator at Ryan\n",
      "\n",
      "fit_keras_tokenizer least fit job title 1: Seeking employment opportunities within Customer Service or Patient Care\n",
      "fit_keras_tokenizer least fit job title 2: Human Development Coordinator at Ryan\n",
      "fit_keras_tokenizer least fit job title 3: SVP, Chief Human Resources Officer, Marketing & Communications, CSR Officer | ENGIE | Houston | The Woodlands | Energy | Global Professional in Human Resources | Senior Professional in Human Resources\n",
      "fit_keras_tokenizer least fit job title 4: Human Development Coordinator at Ryan\n",
      "fit_keras_tokenizer least fit job title 5: Human Development Coordinator at Ryan\n",
      "\n",
      "fit_glove least fit job title 1: 2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional\n",
      "fit_glove least fit job title 2: 2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional\n",
      "fit_glove least fit job title 3: 2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional\n",
      "fit_glove least fit job title 4: 2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional\n",
      "fit_glove least fit job title 5: 2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional\n",
      "\n",
      "fit_word2vec least fit job title 1: Human Development Coordinator at Ryan\n",
      "fit_word2vec least fit job title 2: Human Development Coordinator at Ryan\n",
      "fit_word2vec least fit job title 3: Human Development Coordinator at Ryan\n",
      "fit_word2vec least fit job title 4: Human Development Coordinator at Ryan\n",
      "fit_word2vec least fit job title 5: Human Development Coordinator at Ryan\n",
      "\n",
      "fit_fasttext least fit job title 1: Seeking employment opportunities within Customer Service or Patient Care\n",
      "fit_fasttext least fit job title 2: Director Human Resources at EY\n",
      "fit_fasttext least fit job title 3: Human Resources professional for the world leader in GIS software\n",
      "fit_fasttext least fit job title 4: Director of Human Resources North America, Groupe Beneteau\n",
      "fit_fasttext least fit job title 5: Senior Human Resources Business Partner at Heil Environmental\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fit_col in fit_columns:\n",
    "    for i, job_title in enumerate(data[data['has_zero_scores'] == 0].sort_values(fit_col).head().job_title.values):\n",
    "        print(f\"{fit_col} least fit job title {i+1}: {job_title}\")\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although these are 'worst' candidates, the job titles do include a part of keywords such as \"Human\", \"seeking\", so they are not entirely irrelevant. Hence, we will discard only the candidates with at least 1 zero fitness score (i.e. has_zero_score==1).\n",
    "\n",
    "This exclusion method can be built into the pipeline so that irrelevant candidates will be excluded from the list of potential candidates before being presented to hiring managers or HR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered = data[ data['has_zero_scores'] != 1 ].drop('has_zero_scores', axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>6. Present initial list of top candidates</b>\n",
    "\n",
    "Here are top 20 'best-fit' candidates and their job titles, after dropping candidates with at least 1 zero fitness score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique job titles of top 20 candidates:\n",
      "['Seeking Human Resources Opportunities'\n",
      " 'Seeking Human Resources Position'\n",
      " 'Aspiring Human Resources Professional'\n",
      " 'Aspiring Human Resources Manager, seeking internship in Human Resources.'\n",
      " 'Aspiring Human Resources Specialist'\n",
      " 'Seeking Human Resources Human Resources Information System and Generalist Positions']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>connection</th>\n",
       "      <th>fit</th>\n",
       "      <th>fit_tfidf</th>\n",
       "      <th>fit_keras_tokenizer</th>\n",
       "      <th>fit_glove</th>\n",
       "      <th>fit_word2vec</th>\n",
       "      <th>fit_fasttext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>Seeking Human Resources Opportunities</td>\n",
       "      <td>Chicago, Illinois</td>\n",
       "      <td>390</td>\n",
       "      <td>4.828453</td>\n",
       "      <td>0.921024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.973559</td>\n",
       "      <td>0.933869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>Seeking Human Resources Opportunities</td>\n",
       "      <td>Chicago, Illinois</td>\n",
       "      <td>390</td>\n",
       "      <td>4.828453</td>\n",
       "      <td>0.921024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.973559</td>\n",
       "      <td>0.933869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99</td>\n",
       "      <td>Seeking Human Resources Position</td>\n",
       "      <td>Las Vegas, Nevada Area</td>\n",
       "      <td>48</td>\n",
       "      <td>4.762808</td>\n",
       "      <td>0.913385</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.953443</td>\n",
       "      <td>0.973559</td>\n",
       "      <td>0.922421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Aspiring Human Resources Professional</td>\n",
       "      <td>Raleigh-Durham, North Carolina Area</td>\n",
       "      <td>44</td>\n",
       "      <td>4.693111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.920193</td>\n",
       "      <td>0.815329</td>\n",
       "      <td>0.957589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>Aspiring Human Resources Professional</td>\n",
       "      <td>Raleigh-Durham, North Carolina Area</td>\n",
       "      <td>44</td>\n",
       "      <td>4.693111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.920193</td>\n",
       "      <td>0.815329</td>\n",
       "      <td>0.957589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21</td>\n",
       "      <td>Aspiring Human Resources Professional</td>\n",
       "      <td>Raleigh-Durham, North Carolina Area</td>\n",
       "      <td>44</td>\n",
       "      <td>4.693111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.920193</td>\n",
       "      <td>0.815329</td>\n",
       "      <td>0.957589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>33</td>\n",
       "      <td>Aspiring Human Resources Professional</td>\n",
       "      <td>Raleigh-Durham, North Carolina Area</td>\n",
       "      <td>44</td>\n",
       "      <td>4.693111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.920193</td>\n",
       "      <td>0.815329</td>\n",
       "      <td>0.957589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>46</td>\n",
       "      <td>Aspiring Human Resources Professional</td>\n",
       "      <td>Raleigh-Durham, North Carolina Area</td>\n",
       "      <td>44</td>\n",
       "      <td>4.693111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.920193</td>\n",
       "      <td>0.815329</td>\n",
       "      <td>0.957589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>58</td>\n",
       "      <td>Aspiring Human Resources Professional</td>\n",
       "      <td>Raleigh-Durham, North Carolina Area</td>\n",
       "      <td>44</td>\n",
       "      <td>4.693111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.920193</td>\n",
       "      <td>0.815329</td>\n",
       "      <td>0.957589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>97</td>\n",
       "      <td>Aspiring Human Resources Professional</td>\n",
       "      <td>Kokomo, Indiana Area</td>\n",
       "      <td>71</td>\n",
       "      <td>4.693111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.920193</td>\n",
       "      <td>0.815329</td>\n",
       "      <td>0.957589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>73</td>\n",
       "      <td>Aspiring Human Resources Manager, seeking inte...</td>\n",
       "      <td>Houston, Texas Area</td>\n",
       "      <td>7</td>\n",
       "      <td>4.609975</td>\n",
       "      <td>0.648168</td>\n",
       "      <td>0.979796</td>\n",
       "      <td>0.982011</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6</td>\n",
       "      <td>Aspiring Human Resources Specialist</td>\n",
       "      <td>Greater New York City Area</td>\n",
       "      <td>1</td>\n",
       "      <td>4.529383</td>\n",
       "      <td>0.830646</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962413</td>\n",
       "      <td>0.822934</td>\n",
       "      <td>0.913391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>24</td>\n",
       "      <td>Aspiring Human Resources Specialist</td>\n",
       "      <td>Greater New York City Area</td>\n",
       "      <td>1</td>\n",
       "      <td>4.529383</td>\n",
       "      <td>0.830646</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962413</td>\n",
       "      <td>0.822934</td>\n",
       "      <td>0.913391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>36</td>\n",
       "      <td>Aspiring Human Resources Specialist</td>\n",
       "      <td>Greater New York City Area</td>\n",
       "      <td>1</td>\n",
       "      <td>4.529383</td>\n",
       "      <td>0.830646</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962413</td>\n",
       "      <td>0.822934</td>\n",
       "      <td>0.913391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>49</td>\n",
       "      <td>Aspiring Human Resources Specialist</td>\n",
       "      <td>Greater New York City Area</td>\n",
       "      <td>1</td>\n",
       "      <td>4.529383</td>\n",
       "      <td>0.830646</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962413</td>\n",
       "      <td>0.822934</td>\n",
       "      <td>0.913391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>60</td>\n",
       "      <td>Aspiring Human Resources Specialist</td>\n",
       "      <td>Greater New York City Area</td>\n",
       "      <td>1</td>\n",
       "      <td>4.529383</td>\n",
       "      <td>0.830646</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962413</td>\n",
       "      <td>0.822934</td>\n",
       "      <td>0.913391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10</td>\n",
       "      <td>Seeking Human Resources Human Resources Inform...</td>\n",
       "      <td>Greater Philadelphia Area</td>\n",
       "      <td>500+</td>\n",
       "      <td>4.260485</td>\n",
       "      <td>0.736714</td>\n",
       "      <td>0.755929</td>\n",
       "      <td>0.953667</td>\n",
       "      <td>0.903856</td>\n",
       "      <td>0.910319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>40</td>\n",
       "      <td>Seeking Human Resources Human Resources Inform...</td>\n",
       "      <td>Greater Philadelphia Area</td>\n",
       "      <td>500+</td>\n",
       "      <td>4.260485</td>\n",
       "      <td>0.736714</td>\n",
       "      <td>0.755929</td>\n",
       "      <td>0.953667</td>\n",
       "      <td>0.903856</td>\n",
       "      <td>0.910319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>53</td>\n",
       "      <td>Seeking Human Resources Human Resources Inform...</td>\n",
       "      <td>Greater Philadelphia Area</td>\n",
       "      <td>500+</td>\n",
       "      <td>4.260485</td>\n",
       "      <td>0.736714</td>\n",
       "      <td>0.755929</td>\n",
       "      <td>0.953667</td>\n",
       "      <td>0.903856</td>\n",
       "      <td>0.910319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>62</td>\n",
       "      <td>Seeking Human Resources Human Resources Inform...</td>\n",
       "      <td>Greater Philadelphia Area</td>\n",
       "      <td>500+</td>\n",
       "      <td>4.260485</td>\n",
       "      <td>0.736714</td>\n",
       "      <td>0.755929</td>\n",
       "      <td>0.953667</td>\n",
       "      <td>0.903856</td>\n",
       "      <td>0.910319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                          job_title  \\\n",
       "0   28              Seeking Human Resources Opportunities   \n",
       "1   30              Seeking Human Resources Opportunities   \n",
       "2   99                   Seeking Human Resources Position   \n",
       "3    3              Aspiring Human Resources Professional   \n",
       "4   17              Aspiring Human Resources Professional   \n",
       "5   21              Aspiring Human Resources Professional   \n",
       "6   33              Aspiring Human Resources Professional   \n",
       "7   46              Aspiring Human Resources Professional   \n",
       "8   58              Aspiring Human Resources Professional   \n",
       "9   97              Aspiring Human Resources Professional   \n",
       "10  73  Aspiring Human Resources Manager, seeking inte...   \n",
       "11   6                Aspiring Human Resources Specialist   \n",
       "12  24                Aspiring Human Resources Specialist   \n",
       "13  36                Aspiring Human Resources Specialist   \n",
       "14  49                Aspiring Human Resources Specialist   \n",
       "15  60                Aspiring Human Resources Specialist   \n",
       "16  10  Seeking Human Resources Human Resources Inform...   \n",
       "17  40  Seeking Human Resources Human Resources Inform...   \n",
       "18  53  Seeking Human Resources Human Resources Inform...   \n",
       "19  62  Seeking Human Resources Human Resources Inform...   \n",
       "\n",
       "                               location connection       fit  fit_tfidf  \\\n",
       "0                     Chicago, Illinois        390  4.828453   0.921024   \n",
       "1                     Chicago, Illinois        390  4.828453   0.921024   \n",
       "2                Las Vegas, Nevada Area         48  4.762808   0.913385   \n",
       "3   Raleigh-Durham, North Carolina Area         44  4.693111   1.000000   \n",
       "4   Raleigh-Durham, North Carolina Area         44  4.693111   1.000000   \n",
       "5   Raleigh-Durham, North Carolina Area         44  4.693111   1.000000   \n",
       "6   Raleigh-Durham, North Carolina Area         44  4.693111   1.000000   \n",
       "7   Raleigh-Durham, North Carolina Area         44  4.693111   1.000000   \n",
       "8   Raleigh-Durham, North Carolina Area         44  4.693111   1.000000   \n",
       "9                  Kokomo, Indiana Area         71  4.693111   1.000000   \n",
       "10                  Houston, Texas Area          7  4.609975   0.648168   \n",
       "11           Greater New York City Area          1  4.529383   0.830646   \n",
       "12           Greater New York City Area          1  4.529383   0.830646   \n",
       "13           Greater New York City Area          1  4.529383   0.830646   \n",
       "14           Greater New York City Area          1  4.529383   0.830646   \n",
       "15           Greater New York City Area          1  4.529383   0.830646   \n",
       "16            Greater Philadelphia Area      500+   4.260485   0.736714   \n",
       "17            Greater Philadelphia Area      500+   4.260485   0.736714   \n",
       "18            Greater Philadelphia Area      500+   4.260485   0.736714   \n",
       "19            Greater Philadelphia Area      500+   4.260485   0.736714   \n",
       "\n",
       "    fit_keras_tokenizer  fit_glove  fit_word2vec  fit_fasttext  \n",
       "0              1.000000   1.000000      0.973559      0.933869  \n",
       "1              1.000000   1.000000      0.973559      0.933869  \n",
       "2              1.000000   0.953443      0.973559      0.922421  \n",
       "3              1.000000   0.920193      0.815329      0.957589  \n",
       "4              1.000000   0.920193      0.815329      0.957589  \n",
       "5              1.000000   0.920193      0.815329      0.957589  \n",
       "6              1.000000   0.920193      0.815329      0.957589  \n",
       "7              1.000000   0.920193      0.815329      0.957589  \n",
       "8              1.000000   0.920193      0.815329      0.957589  \n",
       "9              1.000000   0.920193      0.815329      0.957589  \n",
       "10             0.979796   0.982011      1.000000      1.000000  \n",
       "11             1.000000   0.962413      0.822934      0.913391  \n",
       "12             1.000000   0.962413      0.822934      0.913391  \n",
       "13             1.000000   0.962413      0.822934      0.913391  \n",
       "14             1.000000   0.962413      0.822934      0.913391  \n",
       "15             1.000000   0.962413      0.822934      0.913391  \n",
       "16             0.755929   0.953667      0.903856      0.910319  \n",
       "17             0.755929   0.953667      0.903856      0.910319  \n",
       "18             0.755929   0.953667      0.903856      0.910319  \n",
       "19             0.755929   0.953667      0.903856      0.910319  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filtered = data_filtered.sort_values(['fit', 'id', 'connection'], ascending=[False, True, True]).reset_index(drop=True)\n",
    "print(f\"Unique job titles of top 20 candidates:\\n{data_filtered.head(20).job_title.unique()}\")\n",
    "data_filtered.head(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>7. Prepare train and test data</b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize job titles using FastText since it's more recent and robust algorithm than the others used in the earlier steps. The transformed word vectors will be used as training features.\n",
    "\n",
    "Set the 'id' column as the index and the 'fit' column as the target. That is, the index of each row will be the id of each candidate and the fitness score of each candidate will be the target, which we want to predict based on (vectorized) job titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features = pd.DataFrame(\n",
    "    get_word_vectors(data_filtered, 'job_title', vectorizer='fasttext',\n",
    "                     to_process_text=True, remove_stopwords=True, lemmatize=False, stem=False)\n",
    ")\n",
    "data_selected = pd.concat([data_filtered, training_features], axis=1).set_index('id')\n",
    "X = data_selected[training_features.columns]\n",
    "y = data_selected['fit']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train and test sets before model training or further transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "random_state = 1\n",
    "X_train, X_test, y_train_fitness, y_test_fitness = split_data(\n",
    "    X, y, test_size, random_state=random_state, oversampling=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert fitness scores into ranks for each of the y_train and y_test data sets separately so that ranks will start from 1 to the number of data points for each data set. For instance, if there are 10 instances in the y_train data set, the rank will range from 1 to 10, not from 2 to 20 with many missing ranks.\n",
    "\n",
    "Also, change the name of the target column from 'fit' to 'rank' since we've got ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train_fitness.rank(method='dense', ascending=False)\n",
    "y_train.name = 'rank'\n",
    "\n",
    "y_test = y_test_fitness.rank(method='dense', ascending=False)\n",
    "y_test.name = 'rank'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>8. Train ranking models</b>\n",
    "\n",
    "Now it's time to train Learning To Rank (LTR) models. LTR models in general take training features and ranks as input and try to learn how a set of objects/instances is ranked based on the training features. In our case, training features will be the job title vectors and the prediction target will be ranks. A major difference between a traditional supervised machine learning model and an LTR model is that the former will produce a prediction for each instance at a time whereas the latter will produce a list of ranks as a whole.\n",
    "\n",
    "We will use NDCG (normalized discounted cumulative gain) as the evaluation metric for the ranking models. NDCG is a measure of the effectiveness of a ranking system, taking into account the position of relevant items in the ranked list. It is based on the idea that items that are higher in the ranking should be given more credit than items that are lower in the ranking - it penalizes highly relevant items ranked lower, which should have appeared higher in the ranked list.\n",
    "\n",
    "##### <b>8-1. XGBoost Ranker</b>\n",
    "\n",
    "XGBoost (eXtreme Gradient Boosting) is a tree based ensemble machine learning algorithm where decision trees are grown sequentially with more weights given to weak learners so that the overall prediction error can be minimized. Thanks to its efficiency and versatility, it is widely used in the field of data science. Thus, we are also going to use a ranker based on XGBoost, that is the XGB Ranker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth stats:\n",
      "                      y_train  y_test\n",
      "Mean (Top 5 rankers)   2.7368  3.3750\n",
      "Mean                  14.5323  6.1250\n",
      "Std                   10.0421  3.2838 \n",
      "\n",
      "Train stats:\n",
      "Mean rank of top 5 candidates based on predictions: 4.4211\n",
      "Mean rank of all candidates based on predictions: 14.5\n",
      "Std rank of all candidates based on predictions: 9.569\n",
      "Mean absolute difference between each pair of rank and predicted rank: 2.9677\n",
      "    rank  pred_rank  abs_diff\n",
      "id                           \n",
      "28   1.0        1.0       0.0\n",
      "30   1.0        1.0       0.0\n",
      "60   3.0        2.0       1.0\n",
      "36   3.0        2.0       1.0\n",
      "49   3.0        2.0       1.0 \n",
      "\n",
      "Test stats:\n",
      "Mean rank of top 5 candidates based on predictions: 3.375\n",
      "Mean rank of all candidates based on predictions: 5.9375\n",
      "Std rank of all candidates based on predictions: 3.8204\n",
      "Mean absolute difference between each pair of rank and predicted rank: 2.5625\n",
      "    rank  pred_rank  abs_diff\n",
      "id                           \n",
      "73   2.0        1.0       1.0\n",
      "25   4.0        2.0       2.0\n",
      "52   4.0        2.0       2.0\n",
      "9    4.0        2.0       2.0\n",
      "39   4.0        2.0       2.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_params = {\n",
    "    'objective': 'rank:pairwise',\n",
    "    'booster': 'gbtree',\n",
    "    'eval_metric': 'ndcg',\n",
    "    'random_state': random_state\n",
    "}\n",
    "\n",
    "xgb_ranker = XGBRanker(**xgb_params)\n",
    "xgb_ranker.fit(X_train, y_train,\n",
    "               group=y_train.value_counts(),\n",
    "               eval_set=[(X_test, y_test)],\n",
    "               eval_group=[y_test.value_counts()]\n",
    "               )\n",
    "\n",
    "stats_df, xgb_train_result, xgb_test_result = get_stats(X_train, X_test, y_train, y_test,\n",
    "                                                        xgb_ranker, target_column=\"rank\", target=\"candidates\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The XGBRanker seems to be performing fine - low mean absolute difference between the real ranks and predicted ranks, also the mean and standard deviation of the predicted ranks are quite close to the stats of the real ranks.\n",
    "\n",
    "##### <b>8-2. LGBM Ranker</b>\n",
    "\n",
    "However, LGBM (Light Gradient Boosting Machine) is becoming more popular as it's even faster than XGBoost without compromising accuracy and it provides more than 100 hyperparameters for users to tune. One of the main differences between LGBM and XGB is that LGBM grow trees leaf-wise whereas XGB grow trees level-wise, resulting in smaller and faster models with LGBM.\n",
    "\n",
    "Thus, let's try this algorithm out and see how it performs compared to the XGB ranker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth stats:\n",
      "                      y_train  y_test\n",
      "Mean (Top 5 rankers)   2.7368  3.3750\n",
      "Mean                  14.5323  6.1250\n",
      "Std                   10.0421  3.2838 \n",
      "\n",
      "Train stats:\n",
      "Mean rank of top 5 candidates based on predictions: 2.8947\n",
      "Mean rank of all candidates based on predictions: 14.1452\n",
      "Std rank of all candidates based on predictions: 10.2363\n",
      "Mean absolute difference between each pair of rank and predicted rank: 2.5484\n",
      "    rank  pred_rank  abs_diff\n",
      "id                           \n",
      "3    2.0        1.0       1.0\n",
      "58   2.0        1.0       1.0\n",
      "46   2.0        1.0       1.0\n",
      "17   2.0        1.0       1.0\n",
      "33   2.0        1.0       1.0 \n",
      "\n",
      "Test stats:\n",
      "Mean rank of top 5 candidates based on predictions: 4.125\n",
      "Mean rank of all candidates based on predictions: 6.0625\n",
      "Std rank of all candidates based on predictions: 3.5491\n",
      "Mean absolute difference between each pair of rank and predicted rank: 2.5625\n",
      "    rank  pred_rank  abs_diff\n",
      "id                           \n",
      "73   2.0        1.0       1.0\n",
      "69   7.0        2.0       5.0\n",
      "25   4.0        3.0       1.0\n",
      "52   4.0        3.0       1.0\n",
      "9    4.0        3.0       1.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lgbm_ranker = LGBMRanker(\n",
    "    boosting_type=\"dart\",\n",
    "    objective=\"lambdarank\",\n",
    "    metric= \"ndcg\",\n",
    "    label_gain =[i for i in range(int(max(y_train.max(), y_test.max())) + 2)],\n",
    "    random_state=random_state\n",
    "    )\n",
    "\n",
    "lgbm_ranker.fit(\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    group=y_train.value_counts(),\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    eval_group=[y_test.value_counts()],\n",
    "    verbose=-1\n",
    "    )\n",
    "\n",
    "_, lgbm_train_result, lgbm_test_result = get_stats(X_train, X_test, y_train, y_test,\n",
    "                                                   lgbm_ranker, target_column=\"rank\", target=\"candidates\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LGBM Ranker does run faster than the XGB Ranker but it does not always produce prediction results that are a lot better than the XGB Ranker, although most of the time it still performs slightly better - e.g. lower mean rank of top 5 candidates.\n",
    "\n",
    "Perhaps hyperparameter tuning could make a big difference, but for the time being we cannot conclude that the LGBM Ranker is better than the XGB Ranker."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>9. Star ideal candidates to star</b>\n",
    "\n",
    "Since we have built our base ranking models, time to proceed to starring ideal candidates and re-rank all candidates based on the stars. Starring one candidate sets this candidate as an ideal candidate for the given role. The list of candidates will be re-ranked each time a candidate or a list of candidates is starred."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>9-1. Get ids of ideal candidates</b>\n",
    "\n",
    "Since we cannot actually take input from HR for starring ideal candidates, randomly choose 5 candidates as ideal candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(random_state)\n",
    "input_ids = random.choices(y.index, k=5)\n",
    "ideal_candidates = sorted([id for id in input_ids])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>9-2. Create a copy of train and test data for re-ranking candidates based on stars</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_updated = X_train.copy()\n",
    "X_test_updated = X_test.copy()\n",
    "y_train_updated = y_train.copy()\n",
    "y_test_updated = y_test.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b>9-3. Add a binary feature 'star' to training features</b>\n",
    "\n",
    "If a candidate is 'starred', the feature value will be 1, otherwise 0. This feature will be a part of training features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_updated['star'] = [1 if idx in ideal_candidates else 0 for idx in X_train_updated.index]\n",
    "X_test_updated['star'] = [1 if idx in ideal_candidates else 0 for idx in X_test_updated.index]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>10. Re-rank all candidates</b>\n",
    "\n",
    "Add 1 to the ranks of all candidates (e.g. rank 1 will become rank 2), and then change the rank of the ideal candidates to 1 in y_train_updated and y_test_updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank of candidate 13 in y_test updated to 1.\n",
      "Rank of candidate 15 in y_train updated to 1.\n",
      "Rank of candidate 52 in y_test updated to 1.\n",
      "Rank of candidate 62 in y_train updated to 1.\n",
      "Rank of candidate 73 in y_test updated to 1.\n"
     ]
    }
   ],
   "source": [
    "y_train_updated, y_test_updated = update_ranks(y_train_updated, y_test_updated, ideal_candidates)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>11. Re-train ranking models based on updated ranks and training features</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Updated) Ground truth stats:\n",
      "                      y_train_updated  y_test_updated\n",
      "Mean (Top 5 rankers)           3.2632          3.0000\n",
      "Mean                          15.0161          6.2500\n",
      "Std                           10.1343          3.9749 \n",
      "\n",
      "(Updated) Train stats:\n",
      "Mean rank of top 5 candidates based on predictions: 3.2105\n",
      "Mean rank of all candidates based on predictions: 15.0806\n",
      "Std rank of all candidates based on predictions: 10.4306\n",
      "Mean absolute difference between each pair of rank and predicted rank: 3.1935\n",
      "    rank  pred_rank  abs_diff\n",
      "id                           \n",
      "3    3.0        1.0       2.0\n",
      "58   3.0        1.0       2.0\n",
      "46   3.0        1.0       2.0\n",
      "17   3.0        1.0       2.0\n",
      "33   3.0        1.0       2.0 \n",
      "\n",
      "(Updated) Test stats:\n",
      "Mean rank of top 5 candidates based on predictions: 4.25\n",
      "Mean rank of all candidates based on predictions: 6.9375\n",
      "Std rank of all candidates based on predictions: 4.1868\n",
      "Mean absolute difference between each pair of rank and predicted rank: 2.5625\n",
      "    rank  pred_rank  abs_diff\n",
      "id                           \n",
      "73   1.0        1.0       0.0\n",
      "52   1.0        2.0       1.0\n",
      "25   5.0        3.0       2.0\n",
      "9    5.0        3.0       2.0\n",
      "39   5.0        3.0       2.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_ranker.fit(X_train_updated, y_train_updated,\n",
    "               group=y_train_updated.value_counts(),\n",
    "               eval_set=[(X_test_updated, y_test_updated)],\n",
    "               eval_group=[y_test_updated.value_counts()]\n",
    "               )\n",
    "\n",
    "stats_df_updated, xgb_train_result_updated, xgb_test_result_updated = get_stats(X_train_updated, X_test_updated,\n",
    "                                                                                y_train_updated, y_test_updated,\n",
    "                                                                                xgb_ranker,\n",
    "                                                                                target_column=\"rank\", target=\"candidates\",\n",
    "                                                                                updated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Updated) Ground truth stats:\n",
      "                      y_train_updated  y_test_updated\n",
      "Mean (Top 5 rankers)           3.2632          3.0000\n",
      "Mean                          15.0161          6.2500\n",
      "Std                           10.1343          3.9749 \n",
      "\n",
      "(Updated) Train stats:\n",
      "Mean rank of top 5 candidates based on predictions: 3.4737\n",
      "Mean rank of all candidates based on predictions: 13.9355\n",
      "Std rank of all candidates based on predictions: 10.1834\n",
      "Mean absolute difference between each pair of rank and predicted rank: 3.629\n",
      "    rank  pred_rank  abs_diff\n",
      "id                           \n",
      "3    3.0        1.0       2.0\n",
      "58   3.0        1.0       2.0\n",
      "46   3.0        1.0       2.0\n",
      "17   3.0        1.0       2.0\n",
      "33   3.0        1.0       2.0 \n",
      "\n",
      "(Updated) Test stats:\n",
      "Mean rank of top 5 candidates based on predictions: 5.375\n",
      "Mean rank of all candidates based on predictions: 6.625\n",
      "Std rank of all candidates based on predictions: 3.2223\n",
      "Mean absolute difference between each pair of rank and predicted rank: 2.625\n",
      "     rank  pred_rank  abs_diff\n",
      "id                            \n",
      "81   10.0        1.0       9.0\n",
      "73    1.0        2.0       1.0\n",
      "99    2.0        3.0       1.0\n",
      "101   4.0        4.0       0.0\n",
      "69    8.0        5.0       3.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lgbm_ranker.fit(\n",
    "    X=X_train_updated,\n",
    "    y=y_train_updated,\n",
    "    group=y_train_updated.value_counts(),\n",
    "    eval_set=[(X_test_updated, y_test_updated)],\n",
    "    eval_group=[y_test_updated.value_counts()],\n",
    "    verbose=-1\n",
    "    )\n",
    "\n",
    "_, lgbm_train_result_updated, lgbm_test_result_updated = get_stats(X_train_updated, X_test_updated,\n",
    "                                                                   y_train_updated, y_test_updated,\n",
    "                                                                   lgbm_ranker,\n",
    "                                                                   target_column=\"rank\", target=\"candidates\",\n",
    "                                                                   updated=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>12. Evaluate results</b>\n",
    "\n",
    "Collate and rearrange statistics for easier evaluation of the results and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_train</th>\n",
       "      <th>y_train_updated</th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_test_updated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mean (Top 5 rankers)</th>\n",
       "      <td>2.7368</td>\n",
       "      <td>3.2632</td>\n",
       "      <td>3.3750</td>\n",
       "      <td>3.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean</th>\n",
       "      <td>14.5323</td>\n",
       "      <td>15.0161</td>\n",
       "      <td>6.1250</td>\n",
       "      <td>6.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Std</th>\n",
       "      <td>10.0421</td>\n",
       "      <td>10.1343</td>\n",
       "      <td>3.2838</td>\n",
       "      <td>3.9749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      y_train  y_train_updated  y_test  y_test_updated\n",
       "Mean (Top 5 rankers)   2.7368           3.2632  3.3750          3.0000\n",
       "Mean                  14.5323          15.0161  6.1250          6.2500\n",
       "Std                   10.0421          10.1343  3.2838          3.9749"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_df_concat = pd.concat([stats_df, stats_df_updated], axis=1)\n",
    "stats_df_concat = stats_df_concat.iloc[:, [0, 2, 1, 3]] # re-arrange columns\n",
    "stats_df_concat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, the stats before and after the starring of ideal candidates (e.g. y_train vs. y_train_updated) didn't vary significantly. In other words, we can consider the ranking models can handle the update of ranks based on stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall mean statistics:\n",
      "           xgb_train  xgb_train_updated  xgb_test  xgb_test_updated  \\\n",
      "rank         14.5323            15.0161    6.1250            6.2500   \n",
      "pred_rank    14.5000            15.0806    5.9375            6.9375   \n",
      "abs_diff      2.9677             3.1935    2.5625            2.5625   \n",
      "\n",
      "           lgbm_train  lgbm_train_updated  lgbm_test  lgbm_test_updated  \n",
      "rank          14.5323             15.0161     6.1250              6.250  \n",
      "pred_rank     14.1452             13.9355     6.0625              6.625  \n",
      "abs_diff       2.5484              3.6290     2.5625              2.625  \n"
     ]
    }
   ],
   "source": [
    "overall_mean_df = get_mean_stats(\n",
    "    xgb_train_result, xgb_train_result_updated,\n",
    "    xgb_test_result, xgb_test_result_updated,\n",
    "    lgbm_train_result, lgbm_train_result_updated,\n",
    "    lgbm_test_result, lgbm_test_result_updated\n",
    "    )\n",
    "print(f\"Overall mean statistics:\\n{overall_mean_df}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 mean statistics:\n",
      "           xgb_train  xgb_train_updated  xgb_test  xgb_test_updated  \\\n",
      "rank          2.7368             3.2632     3.375              3.00   \n",
      "pred_rank     4.4211             3.2105     3.375              4.25   \n",
      "abs_diff      2.2105             1.9474     2.250              2.75   \n",
      "\n",
      "           lgbm_train  lgbm_train_updated  lgbm_test  lgbm_test_updated  \n",
      "rank           2.7368              3.2632      3.375              3.000  \n",
      "pred_rank      2.8947              3.4737      4.125              5.375  \n",
      "abs_diff       1.4211              3.3684      2.000              2.375  \n"
     ]
    }
   ],
   "source": [
    "top5_mean_df = get_mean_stats(\n",
    "    xgb_train_result[xgb_train_result[\"rank\"]<=5], xgb_train_result_updated[xgb_train_result_updated[\"rank\"]<=5],\n",
    "    xgb_test_result[xgb_test_result[\"rank\"]<=5], xgb_test_result_updated[xgb_test_result_updated[\"rank\"]<=5],\n",
    "    lgbm_train_result[lgbm_train_result[\"rank\"]<=5], lgbm_train_result_updated[lgbm_train_result_updated[\"rank\"]<=5],\n",
    "    lgbm_test_result[lgbm_test_result[\"rank\"]<=5], lgbm_test_result_updated[lgbm_test_result_updated[\"rank\"]<=5]\n",
    "    )\n",
    "print(f\"Top 5 mean statistics:\\n{top5_mean_df}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the overall and top 5 mean statistics, the performance of each model in terms of the mean absolute difference between real ranks and predicted ranks could vary based on the particular selection of ideal candidates, but on the whole both models appear to be able to handle the re-ranking of candidates pretty well. That is, the performances didn't dramatically decrease (or increase) after the re-training based on the starring of ideal candidates."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>13. Save models for later use</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model saved: c:\\Users\\Admin\\Documents\\GitHub\\Apziva\\YKTXOBGWLuUXdzbs\\xgb_ranker_trained.sav\n",
      "Trained model saved: c:\\Users\\Admin\\Documents\\GitHub\\Apziva\\YKTXOBGWLuUXdzbs\\lgbm_ranker_trained.sav\n"
     ]
    }
   ],
   "source": [
    "save_model(xgb_ranker, \"xgb_ranker_trained\")\n",
    "save_model(lgbm_ranker, \"lgbm_ranker_trained\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>14. Conclusion</b>\n",
    "\n",
    "goal and success metrics\n",
    "\n",
    "\n",
    "was able to get fitness scores using various word embedding techniques and use them to train LTR models.\n",
    "evaluated the model predictions - looked at means, sts, whole candidates, only top 5, etc. \n",
    "the process of re-ranking (or starring) candidates was built into the machine learning pipeline and the models were able to handle updated ranks and produce reasonable predictions based on the updated ranks.\n",
    "\n",
    "\n",
    "other objectives\n",
    "- We are interested in a robust algorithm, tell us how your solution works and show us how your ranking gets better with each starring action.\n",
    "my solutions do not soley rely on a single ranking algorithm or word embedding technique.\n",
    "\n",
    "the fit column is calculated based on 5 different fitness metrics. each of those used the cosine similarity between the vector of job title and the vector of keywords for each candidate, where job titles that are highly related to keywords (e.g. human resources) will get higher cosine similarity scores.\n",
    "\n",
    "as for rankers, XGB ranker and lgbm ranker can complement each other - can use both models for selecting top candidates to reduce the cahnge of missing high potential candidates.\n",
    "\n",
    "- How can we filter out candidates which in the first place should not be in this list?\n",
    "based on how fitness scores were calculated in this notebook, candidates with a fitness score of 0 from the fit column can be dropped. If no candidates have a fitness score of 0, can drop candidates with at least 1 '0 fitness score' from any of the fit columns such as fit_tfidf, fit_glove - this information is stored in the has_zero_scores column.\n",
    "\n",
    "looking at the job titles of those candidates, they did look irrelevant.\n",
    "\n",
    "- Can we determine a cut-off point that would work for other roles without losing high potential candidates?\n",
    "based on how fitness scores were calculated in this notebook, candidates with a fitness score of 0 from the fit column can be dropped. If no candidates have a fitness score of 0, can drop candidates with at least 1 '0 fitness score' from any of the fit columns such as fit_tfidf, fit_glove - this information is stored in the has_zero_scores column.\n",
    "\n",
    "- Do you have any ideas that we should explore so that we can even automate this procedure to prevent human bias?\n",
    "We could automate most of the processes but I don't think we can (or should) automate the manual review and starring of ideal candidates. From my point of view it is imperative to have some sort of human intervention at some point, in this application is the manual review and starring of candidates. Without such supervison, the machine (i.e. the automated procedure) might be effecient but we can never guarantee that it will not be biased at all. Even with the state-of-art technologies and algorithms, I don't believe that there is any machine or algorithm that is entirely bias-free.\n",
    "To conclude, we can automate most part of the procedure, but I would leave out the starring operation to be manually done by human, which could actually help reduce bias or discover subtle errors in the algorithms.\n",
    "\n",
    "10-1. Learning To Rank (LTR) models - XGB Ranker, LGBM Ranker\n",
    "results - the model performance, etc. mention goals and success metrics mentioned in the project description (README).\n",
    "\n",
    "10-2. Word embeddings and vectorizations - TF-IDF, Tokenizer, GloVe, Word2Vec, FastText\n",
    "\n",
    "\n",
    "as a whole, think of the notebook as a story. when a person (interviewer) reads the notebook, they should be able to get what you are trying to do and present.\n",
    "\n",
    "send an email if I submit before next session.\n",
    "\n",
    "\n",
    "\n",
    "10-5. Application\n",
    "the solution can be used in ## situations, business cases, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a4868653bb6f8972e87e4c446ab8a445a15b25dedb8594cc74c480f8152ea86a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
